{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNR4l_n3dqz9",
        "outputId": "ac0ed260-902b-48b4-e560-6e40ca2fc3ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rlcard in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.10/dist-packages (from rlcard) (1.22.4)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from rlcard) (2.3.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install rlcard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DyjR-wweEsR"
      },
      "outputs": [],
      "source": [
        "import rlcard\n",
        "import numpy as np\n",
        "from rlcard.games.limitholdem import Dealer\n",
        "from rlcard.games.base import Card\n",
        "from rlcard.utils.utils import print_card\n",
        "import random\n",
        "import tkinter as tk #loads standard python GUI libraries\n",
        "import time\n",
        "from IPython.display import Markdown\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI_ikXCPpJKu"
      },
      "source": [
        "Readme Section: In order to initialize a PokerGame instance you either use the user friendly built-in InitPlayers (pokergame.InitPlayers()) or you put the player types directly during the instantiaton of the class  e.g:\n",
        "  \n",
        "  pk = PokerGame(HumanAgent(), RandomAgent(), feedback)\n",
        "\n",
        "feedback is an optional argument if you want the game to print info like who won, what actions are taken etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CmgU03lwojDa"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Get the coresponding number of a rank\n",
        "def rank_int(rank):\n",
        "    if rank == '':\n",
        "        return -1\n",
        "    elif rank == 'T':\n",
        "        return 1\n",
        "    elif rank == 'J':\n",
        "        return 2\n",
        "    elif rank == 'Q':\n",
        "        return 3\n",
        "    elif rank == 'K':\n",
        "        return 4\n",
        "    elif rank == 'A':\n",
        "        return 5\n",
        "    return None\n",
        "\n",
        "\n",
        "#A dealer class\n",
        "class PokerDealer:\n",
        "    #Initialize\n",
        "    def __init__(self, np_random):\n",
        "        self.deck = self.init_deck()\n",
        "        self.np_random = np_random\n",
        "        self.shuffle()\n",
        "        self.pot = 0\n",
        "\n",
        "    #Initialize a deck of 20 cards(10,J,Q,K,Î‘)\n",
        "    def init_deck(self):\n",
        "        suit_list = ['S', 'H', 'D', 'C']\n",
        "        rank_list = ['T', 'J', 'Q', 'K', 'A']\n",
        "        res = [Card(suit, rank) for suit in suit_list for rank in rank_list]\n",
        "        return res\n",
        "\n",
        "    #Shuffle the cards\n",
        "    def shuffle(self):\n",
        "         self.np_random.shuffle(self.deck)\n",
        "\n",
        "    #Deal one card from the deck\n",
        "    def deal_card(self):\n",
        "        return self.deck.pop()\n",
        "\n",
        "\n",
        "#A judger class\n",
        "class PokerJudger:\n",
        "    #Initialize\n",
        "    def __init__(self, np_random):\n",
        "        self.np_random = np_random\n",
        "\n",
        "    #Judge the winner/winners of the game\n",
        "    @staticmethod\n",
        "    def judge_game(players, public_cards):\n",
        "        winners = [0] * len(players)\n",
        "        ranks = []\n",
        "        for player in players:\n",
        "            ranks.append(rank_int(player.hand.rank))\n",
        "        max_rank = max(ranks)\n",
        "\n",
        "        #If any of the players fold, the other player wins\n",
        "        for i, player in enumerate(players):\n",
        "           if players[i].status == 'folded':\n",
        "               winners[i] = 1\n",
        "\n",
        "        #If any of the players match both the public cards(3 of a kind) wins\n",
        "        if sum(winners) < 1:\n",
        "           for i, player in enumerate(players):\n",
        "              if players[i].hand.rank == public_cards[0].rank and players[i].hand.rank == public_cards[1].rank:\n",
        "                winners[i] = 1\n",
        "\n",
        "        #Else, if any of the players match one public card(pair) wins. If both do, the player with the highest card rank wins\n",
        "        if sum(winners) < 1:\n",
        "            if (players[0].hand.rank == public_cards[0].rank or players[0].hand.rank == public_cards[1].rank) \\\n",
        "            and (players[1].hand.rank == public_cards[0].rank or players[1].hand.rank == public_cards[1].rank):\n",
        "                  for i, player in enumerate(players):\n",
        "                     if rank_int(players[i].hand.rank) == max_rank:\n",
        "                         winners[i] = 1\n",
        "\n",
        "            for i, player in enumerate(players):\n",
        "               if players[i].hand.rank == public_cards[0].rank or players[i].hand.rank == public_cards[1].rank:\n",
        "                   winners[i] = 1\n",
        "\n",
        "        #If non of the above conditions, the winner player is the one with the highest card rank\n",
        "        if sum(winners) < 1:\n",
        "           for i, player in enumerate(players):\n",
        "              if rank_int(players[i].hand.rank) == max_rank:\n",
        "                winners[i] = 1\n",
        "\n",
        "        #Compute the total chips\n",
        "        total = 0\n",
        "        for p in players:\n",
        "            total += p.in_chips\n",
        "        each_win = float(total) / sum(winners)\n",
        "\n",
        "        payoffs = []\n",
        "        for i, _ in enumerate(players):\n",
        "            if winners[i] == 1:\n",
        "                payoffs.append(each_win - players[i].in_chips)\n",
        "            else:\n",
        "                payoffs.append(float(-players[i].in_chips))\n",
        "\n",
        "        return payoffs\n",
        "\n",
        "\n",
        "#A random agent class to play against\n",
        "class RandomAgent:\n",
        "  #Initialize\n",
        "  def __init__(self):\n",
        "    self.in_chips = 0\n",
        "    self.win_chips = 0\n",
        "    self.hand = None\n",
        "    self.status = 'alive'\n",
        "\n",
        "  #Predict the action given the curent state\n",
        "  def step(self, state):\n",
        "    legal_actions = state[1]\n",
        "    while True:\n",
        "      random_number = np.random.randint(4)\n",
        "      if legal_actions[random_number] == 1:   #choose a legal action randomly\n",
        "        break\n",
        "    if random_number == 0:\n",
        "      return 'call'\n",
        "    elif random_number == 1:\n",
        "      return 'fold'\n",
        "    elif random_number == 2:\n",
        "      return 'check'\n",
        "    else:\n",
        "      return 'raise'\n",
        "\n",
        "\n",
        "#A threshold agent class to play against\n",
        "class ThresholdAgent:\n",
        "  #Initialize\n",
        "  def __init__(self):\n",
        "      self.in_chips = 0\n",
        "      self.win_chips = 0\n",
        "      self.hand = None\n",
        "      self.status = 'alive'\n",
        "\n",
        "  #Check for pair or 3 of a kind\n",
        "  def has_pair(self, hand, public_hand):\n",
        "      for i in public_hand:\n",
        "        if hand.rank == i.rank:\n",
        "          return True\n",
        "      return False\n",
        "\n",
        "  #Predict the action given the curent state\n",
        "  def step(self, state):\n",
        "      public_cards = state[0]\n",
        "      legal_actions = state[1]\n",
        "      #In the 1st round (public cards not yet revealed) raise or call in case of K or A\n",
        "      if (public_cards[0] == None) and (self.hand.rank == 'K' or self.hand.rank == 'A'):\n",
        "        if legal_actions[3] == 1:\n",
        "          return 'raise'\n",
        "        else:\n",
        "          return 'call'\n",
        "      #In the 2nd round (public cards revealed) raise or call in case of a pair or 3 of a kind\n",
        "      elif (public_cards[0] != None) and (self.has_pair(self.hand, public_cards)):\n",
        "        if legal_actions[3] == 1:\n",
        "          return 'raise'\n",
        "        else:\n",
        "          return 'call'\n",
        "      #Else, at any round check or fold\n",
        "      elif legal_actions[2] == 1:\n",
        "        return 'check'\n",
        "      else:\n",
        "        return 'fold'\n",
        "\n",
        "\n",
        "#A human agent class to be able to play interactively against random or threshold agent\n",
        "class HumanAgent:\n",
        "  #Initialize\n",
        "  def __init__(self):\n",
        "    self.in_chips = 0\n",
        "    self.win_chips = 0\n",
        "    self.hand = None\n",
        "    self.status = 'alive'\n",
        "\n",
        "  #Predict the action given the curent state\n",
        "  def step(self, state):\n",
        "    legal_actions = state[1]\n",
        "    str_legal_actions = list()\n",
        "    actions_dic = {\"0\": \"call\", \"1\": \"fold\", \"2\": \"check\", \"3\": \"raise\"}\n",
        "    for index, status in enumerate(legal_actions):\n",
        "      if status == 1:\n",
        "        str_legal_actions.append(actions_dic[str(index)])   #collect the legal actions as a list of strings\n",
        "    public_cards = state[0]\n",
        "    print(\"\\n-----Your Hand-----\")\n",
        "    print_card(self.hand.get_index())\n",
        "\n",
        "    if public_cards[0] != None:\n",
        "      print(\"-------Public Cards---------\")\n",
        "      print_card(public_cards[0].get_index())\n",
        "      print_card(public_cards[1].get_index())\n",
        "\n",
        "    action = input(\"Please enter an action (call, fold, check or raise): \")     #read the user's input(only legal actions acceptable)\n",
        "    while action not in str_legal_actions:\n",
        "      print(\"This action is illegal or invalid.\")\n",
        "      action = input(\"Please enter an action (call, fold, check or raise): \")\n",
        "\n",
        "    return action\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PokerGame:\n",
        "  #Initialize\n",
        "  def __init__(self, player1 = None, player2 = None, feedback = False):\n",
        "    #Players, judger, dealer initializations\n",
        "    self.np_random = np.random.RandomState()\n",
        "    self.dealer = None\n",
        "    self.judger = PokerJudger(self.np_random)\n",
        "    self.player1 = player1\n",
        "    self.player2 = player2\n",
        "    #policy1 and 2 are True when Player 1 is PolicyAgent or player 2 is PolicyAgent\n",
        "    self.policy1 = False\n",
        "    self.policy2 = False\n",
        "    #is True only when player1 is Policy and player2 is Threshold\n",
        "    self.threshold2 = False\n",
        "    self.feedback = feedback\n",
        "    self.raise_r1 = 0\n",
        "    #Bettings and cards initializations\n",
        "    self.ante = 0.5\n",
        "    self.bet = 1\n",
        "    self.rounds = 2\n",
        "    self.public_cards = [None, None]\n",
        "    self.actions_dic = {0: \"call\", 1: \"fold\", 2: \"check\", 3: \"raise\"}\n",
        "\n",
        "    ########## Q L E A R N I N G ############\n",
        "    self.states = []\n",
        "    self.actions = []\n",
        "    self.rewards = []\n",
        "    self.q1 = False\n",
        "    self.q2 = False\n",
        "\n",
        "  #Initialize Players (type, ID)\n",
        "  def InitPlayers(self):\n",
        "\n",
        "    #Choose what type of player you are\n",
        "    while True:\n",
        "      player_type = input(\"Choose your player type:(Random, Human, Threshold, Policy, QLearning) \")\n",
        "      if  player_type in [\"Random\", \"Threshold\",  \"Policy\", \"Human\", \"QLearning\"]:\n",
        "          break\n",
        "      else:\n",
        "          print(\"Invalid player type\")\n",
        "\n",
        "    #Choose to play against random or threshold agent\n",
        "    while True:\n",
        "      opp = input(\"Choose opponent type (Random, Threshold, Human, Policy, QLearning): \")\n",
        "      if opp in [\"Random\", \"Threshold\",  \"Policy\", \"Human\", \"QLearning\"]:\n",
        "          break\n",
        "      else:\n",
        "          print(\"Invalid opponent type.\")\n",
        "\n",
        "    #Define player1 and player2 from user's inputs\n",
        "    if id == \"1\" and player_type == \"Human\":\n",
        "      self.player1 = HumanAgent()\n",
        "      if opp == \"Random\":\n",
        "          self.player2 = RandomAgent()\n",
        "      elif opp == \"Threshold\":\n",
        "          self.player2 = ThresholdAgent()\n",
        "      elif opp == \"Human\":\n",
        "          self.player2 = HumanAgent()\n",
        "      elif opp == \"Policy\" :\n",
        "          self.player2 = PolicyAgent(2, \"Random\")\n",
        "      elif opp == \"QLearning\" :\n",
        "          self.player2 = QLearningAgent()\n",
        "    elif id == \"1\" and player_type == \"Random\":\n",
        "      self.player1 = RandomAgent()\n",
        "      if opp == \"Random\":\n",
        "          self.player2 = RandomAgent()\n",
        "      elif opp == \"Threshold\":\n",
        "          self.player2 = ThresholdAgent()\n",
        "      elif opp == \"Human\":\n",
        "          self.player2 = HumanAgent()\n",
        "      elif opp == \"Policy\" :\n",
        "          self.player2 = PolicyAgent(2, \"Random\")\n",
        "      elif opp == \"QLearning\" :\n",
        "          self.player2 = QLearningAgent()\n",
        "    elif id == \"1\" and player_type == \"Threshold\":\n",
        "      self.player1 = ThresholdAgent()\n",
        "      if opp == \"Random\":\n",
        "          self.player2 = RandomAgent()\n",
        "      elif opp == \"Threshold\":\n",
        "          self.player2 = ThresholdAgent()\n",
        "      elif opp == \"Human\":\n",
        "          self.player2 = HumanAgent()\n",
        "      elif opp == \"Policy\" :\n",
        "          self.player2 = PolicyAgent(2, \"Random\")\n",
        "      elif opp == \"QLearning\" :\n",
        "          self.player2 = QLearningAgent()\n",
        "    elif id == \"1\" and player_type == \"Policy\":\n",
        "      self.policy1 = True\n",
        "      self.player1 = PolicyAgent(1, \"Random\")\n",
        "      if opp == \"Random\":\n",
        "          self.player2 = RandomAgent()\n",
        "      elif opp == \"Threshold\":\n",
        "          self.threshold2 == True\n",
        "          self.player2 = ThresholdAgent()\n",
        "      elif opp == \"Human\":\n",
        "          self.player2 = HumanAgent()\n",
        "      elif opp == \"Policy\" :\n",
        "          self.player2 = PolicyAgent(2, \"Random\")\n",
        "      elif opp == \"QLearning\" :\n",
        "          self.player2 = QLearningAgent()\n",
        "    elif id == \"1\" and player_type == \"QLearning\":\n",
        "      self.q1 = True\n",
        "      self.player1 = QLearningAgent()\n",
        "      if opp == \"Random\":\n",
        "          self.player2 = RandomAgent()\n",
        "      elif opp == \"Threshold\":\n",
        "          self.player2 = ThresholdAgent()\n",
        "      elif opp == \"Human\":\n",
        "          self.player2 = HumanAgent()\n",
        "      elif opp == \"Policy\" :\n",
        "          self.player2 = PolicyAgent(2, \"Random\")\n",
        "      elif opp == \"QLearning\" :\n",
        "          self.player2 = QLearningAgent()\n",
        "    elif id == \"2\" and player_type == \"Human\":\n",
        "      self.player2 = HumanAgent()\n",
        "      if opp == \"Random\":\n",
        "          self.player1 = RandomAgent()\n",
        "      elif opp == \"Threshold\":\n",
        "          self.player1 = ThresholdAgent()\n",
        "      elif opp == \"Human\":\n",
        "          self.player1 = HumanAgent()\n",
        "      elif opp == \"Policy\" :\n",
        "          self.player1 = PolicyAgent(1, \"Random\")\n",
        "      elif opp == \"QLearning\" :\n",
        "          self.player2 = QLearningAgent()\n",
        "    elif id == \"2\" and player_type == \"Random\":\n",
        "      self.player2 = RandomAgent()\n",
        "      if opp == \"Random\":\n",
        "          self.player1 = RandomAgent()\n",
        "      elif opp == \"Threshold\":\n",
        "          self.player1 = ThresholdAgent()\n",
        "      elif opp == \"Human\":\n",
        "          self.player1 = HumanAgent()\n",
        "      elif opp == \"Policy\" :\n",
        "          self.player1 = PolicyAgent(1, \"Random\")\n",
        "      elif opp == \"QLearning\" :\n",
        "          self.player2 = QLearningAgent()\n",
        "    elif id == \"2\" and player_type == \"Threshold\":\n",
        "      self.player2 = ThresholdAgent()\n",
        "      if opp == \"Random\":\n",
        "          self.player1 = RandomAgent()\n",
        "      elif opp == \"Threshold\":\n",
        "          self.player1 = ThresholdAgent()\n",
        "      elif opp == \"Human\":\n",
        "          self.player1 = HumanAgent()\n",
        "      elif opp == \"Policy\" :\n",
        "          self.player1 = PolicyAgent(1, \"Random\")\n",
        "      elif opp == \"QLearning\" :\n",
        "          self.player2 = QLearningAgent()\n",
        "    elif id == \"2\" and player_type == \"Policy\":\n",
        "      self.policy2 = True\n",
        "      self.player2 = PolicyAgent(2, \"Random\")\n",
        "      if opp == \"Random\":\n",
        "          self.player1 = RandomAgent()\n",
        "      elif opp == \"Threshold\":\n",
        "          self.player1 = ThresholdAgent()\n",
        "      elif opp == \"Human\":\n",
        "          self.player1 = HumanAgent()\n",
        "      elif opp == \"Policy\" :\n",
        "          self.player1 = PolicyAgent(1, \"Random\")\n",
        "      elif opp == \"QLearning\" :\n",
        "          self.player2 = QLearningAgent()\n",
        "    elif id == \"2\" and player_type == \"QLearning\":\n",
        "      self.q2 = True\n",
        "      self.player2 = QLearningAgent()\n",
        "      if opp == \"Random\":\n",
        "          self.player2 = RandomAgent()\n",
        "      elif opp == \"Threshold\":\n",
        "          self.player2 = ThresholdAgent()\n",
        "      elif opp == \"Human\":\n",
        "          self.player2 = HumanAgent()\n",
        "      elif opp == \"Policy\" :\n",
        "          self.player2 = PolicyAgent(2, \"Random\")\n",
        "      elif opp == \"QLearning\" :\n",
        "          self.player2 = QLearningAgent()\n",
        "    else:\n",
        "      print(\"Something went wrong\")\n",
        "\n",
        "\n",
        "  #Play one game of poker\n",
        "  def playGame(self):\n",
        "    #Initialization of Dealer\n",
        "    self.dealer =  PokerDealer(self.np_random)\n",
        "    #Hands and chips initializations\n",
        "    self.player1.in_chips = 0\n",
        "    self.player2.in_chips = 0\n",
        "    self.player1.in_chips += self.ante\n",
        "    self.player2.in_chips += self.ante\n",
        "    self.player1.win_chips = 0\n",
        "    self.player2.win_chips = 0\n",
        "    self.player1.hand = self.dealer.deal_card()\n",
        "    self.player2.hand = self.dealer.deal_card()\n",
        "    self.player1.status=\"alive\"\n",
        "    self.player2.status=\"alive\"\n",
        "    self.raise_r1 = 0\n",
        "    pre_public_cards = [self.dealer.deal_card(), self.dealer.deal_card()]\n",
        "    if(self.policy1 == True):\n",
        "      self.player1.train(pre_public_cards)\n",
        "\n",
        "    if(self.policy2 == True):\n",
        "      self.player2.train(pre_public_cards)\n",
        "    #2 rounds in total\n",
        "    for i in range(self.rounds):\n",
        "        if self.feedback:\n",
        "            print(\"\\nRound \"+str(i+1))\n",
        "        call_status = 0\n",
        "        check_status = 1\n",
        "        fold_status = 0\n",
        "        raise_status = 1\n",
        "        raise_count = 0\n",
        "        raise_sum = 0\n",
        "\n",
        "\n",
        "        #Action 1 for player 1\n",
        "        legal_actions = [call_status, fold_status, check_status, raise_status]\n",
        "        #check if player 1 is Q agent to use appropriate step function\n",
        "        if(self.q1 == True ):\n",
        "          state = (self.player1.hand, self.public_cards, i, legal_actions)\n",
        "          self.states.append(state)\n",
        "          #check whether q agent is in training phase or in testing phase\n",
        "          if(self.player1.q_learn == True):\n",
        "            action1 = self.player1.step_train(state)\n",
        "          else:\n",
        "            action1 = self.player1.step_test(state)\n",
        "          self.actions.append(action1)\n",
        "        #check whether player1 is policy agent and choose action from current state using the policy table (pi)\n",
        "        elif(self.policy1 == True):\n",
        "            if(i==0):\n",
        "                action_num = self.player1.pi.get(0)\n",
        "            elif(self.raise_r1==0):\n",
        "                action_num = self.player1.pi.get(3)\n",
        "            elif(self.raise_r1==1):\n",
        "                action_num = self.player1.pi.get(4)\n",
        "            else:\n",
        "                action_num = self.player1.pi.get(5)\n",
        "\n",
        "            action1 = self.actions_dic.get(action_num)\n",
        "        #in every other case use the step function of the agent\n",
        "        else:\n",
        "            action1 = self.player1.step([self.public_cards, legal_actions])\n",
        "\n",
        "        if action1 == 'raise':\n",
        "            if(self.q1 == True and self.player1.q_learn == True):\n",
        "                self.rewards.append(0)\n",
        "            #Update legal action space\n",
        "            call_status = 1\n",
        "            check_status = 0\n",
        "            fold_status = 1\n",
        "            raise_status = 1\n",
        "            raise_count += 1\n",
        "            raise_sum += self.bet\n",
        "            self.player1.in_chips += raise_sum\n",
        "            legal_actions = [call_status, fold_status, check_status, raise_status]\n",
        "            if self.feedback:\n",
        "                print(\"Player 1 has raised.\")\n",
        "        elif action1 == 'check':\n",
        "            if(self.q1 == True and self.player1.q_learn == True):\n",
        "                self.rewards.append(0)\n",
        "            fold_status = 0\n",
        "            call_status = 0\n",
        "            check_status = 1\n",
        "            raise_status = 1\n",
        "            legal_actions = [call_status, fold_status, check_status, raise_status]\n",
        "            if self.feedback:\n",
        "                print(\"Player 1 has checked.\")\n",
        "        else:\n",
        "            print(\"Invalid Action\")\n",
        "            exit(0)\n",
        "\n",
        "        #Action for player 2\n",
        "\n",
        "        #Check whether player 2 is q agent\n",
        "        if(self.q2 == True):\n",
        "          state = (self.player2.hand, self.public_cards, i, legal_actions)\n",
        "          self.states.append(state)\n",
        "          #check if player 2 is in training phase or not\n",
        "          if self.player2.q_learn == True:\n",
        "            action2 = self.player2.step_train(state)\n",
        "          else:\n",
        "            action2 = self.player2.step_test(state)\n",
        "          self.actions.append(action2)\n",
        "          #print(state)\n",
        "\n",
        "        #check whether player2 is policy agent and access its policy table to get correct action from current state\n",
        "        elif(self.policy2 == True):\n",
        "            if(action1 == 'raise' and i == 0):\n",
        "                action_num = self.player2.pi.get(1)\n",
        "            elif(action1 == 'check' and i == 0):\n",
        "                action_num = self.player2.pi.get(0)\n",
        "            elif(self.raise_r1 == 0 and action1 == 'raise'):\n",
        "                action_num = self.player2.pi.get(3)\n",
        "            elif(self.raise_r1 == 0 and action1 == 'check'):\n",
        "                action_num = self.player2.pi.get(2)\n",
        "            elif(self.raise_r1 == 1 and action1 == 'raise'):\n",
        "                action_num = self.player2.pi.get(5)\n",
        "            elif(self.raise_r1 == 1 and action1 == 'check'):\n",
        "                action_num = self.player2.pi.get(4)\n",
        "            elif(self.raise_r1 == 2 and action1 == 'raise'):\n",
        "                action_num = self.player2.pi.get(7)\n",
        "            elif(self.raise_r1 == 2 and action1 == 'check'):\n",
        "                action_num = self.player2.pi.get(6)\n",
        "            else:\n",
        "                print(\"Something went wrong\")\n",
        "                exit(0)\n",
        "            action2 = self.actions_dic.get(action_num)\n",
        "        else:\n",
        "            action2 = self.player2.step([self.public_cards, legal_actions])\n",
        "\n",
        "        if action2 == 'raise':\n",
        "            if(self.q2 == True):\n",
        "                self.rewards.append(0)\n",
        "            raise_count += 1\n",
        "            check_status = 0\n",
        "            raise_status = 0\n",
        "            fold_status = 1\n",
        "            call_status = 1\n",
        "            raise_sum += self.bet\n",
        "            self.player2.in_chips += raise_sum\n",
        "            legal_actions = [call_status, fold_status, check_status, raise_status]\n",
        "            if self.feedback:\n",
        "                print(\"Player 2 has raised.\")\n",
        "        elif action2 == 'call':\n",
        "            if(self.q2 == True):\n",
        "                self.rewards.append(0)\n",
        "            self.player2.in_chips += raise_sum\n",
        "            if self.feedback:\n",
        "                print(\"Player 2 has called.\")\n",
        "        elif action2 == 'fold':\n",
        "            self.player1.win_chips = self.player2.in_chips\n",
        "            self.player2.win_chips = -self.player2.in_chips\n",
        "            self.player2.status = 'folded'\n",
        "            if(self.q2 == True ):\n",
        "                self.rewards.append(self.player2.win_chips)\n",
        "            if self.feedback:\n",
        "                print(\"Player 2 has folded.\")\n",
        "            break\n",
        "        else:\n",
        "            if(self.q2 == True):\n",
        "                self.rewards.append(0)\n",
        "            if self.feedback:\n",
        "                print(\"Player 2 has checked.\")\n",
        "\n",
        "\n",
        "\n",
        "        #Action for player 1 (response to possible raise)\n",
        "        if action1 + action2 == 'raiseraise' or action1 + action2 == 'checkraise':\n",
        "            #Checking what agent type is Player 1\n",
        "            if(self.q1 == True):\n",
        "                state = (self.player1.hand, self.public_cards, i, legal_actions)\n",
        "                self.states.append(state)\n",
        "                if(self.player1.q_learn == True):\n",
        "                    action3 = self.player1.step_train(state)\n",
        "                else:\n",
        "                    action3 = self.player1.step_test(state)\n",
        "                self.actions.append(action3)\n",
        "            elif(self.policy1 == True and self.threshold2 == True):\n",
        "                if(i==0 and action1 + action2 == 'checkraise'):\n",
        "                    action_num = self.player1.pi.get(1)\n",
        "\n",
        "                elif(i==0 and action1 + action2 == 'raiseraise'):\n",
        "                    action_num = self.player1.pi.get(2)\n",
        "\n",
        "                elif(self.raise_r1==0 and action1 + action2 == 'checkraise'):\n",
        "                    action_num = self.player1.pi.get(6)\n",
        "\n",
        "                elif(self.raise_r1==0 and action1 + action2 == 'raiseraise'):\n",
        "                    action_num = self.player1.pi.get(7)\n",
        "\n",
        "                elif(self.raise_r1==1 and action1 + action2 == 'checkraise'):\n",
        "                    action_num = self.player1.pi.get(8)\n",
        "\n",
        "                elif(self.raise_r1==1 and action1 + action2 == 'raiseraise'):\n",
        "                    action_num = self.player1.pi.get(9)\n",
        "\n",
        "                elif(self.raise_r1==1 and action1 + action2 == 'checkraise'):\n",
        "                    action_num = self.player1.pi.get(9)\n",
        "\n",
        "                elif(self.raise_r1==2 and action1 + action2 == 'raiseraise'):\n",
        "                    action_num = self.player1.pi.get(10)\n",
        "\n",
        "                #print(action_num)\n",
        "                action3 = self.actions_dic.get(action_num)\n",
        "            elif(self.policy1 == True):\n",
        "                if(i==0 and action1 + action2 == 'checkraise'):\n",
        "                    action_num = self.player1.pi.get(1)\n",
        "\n",
        "                elif(i==0 and action1 + action2 == 'raiseraise'):\n",
        "                    action_num = self.player1.pi.get(2)\n",
        "\n",
        "                elif(self.raise_r1==0 and action1 + action2 == 'checkraise'):\n",
        "                    action_num = self.player1.pi.get(6)\n",
        "\n",
        "                elif(self.raise_r1==1 and action1 + action2 == 'checkraise'):\n",
        "                    action_num = self.player1.pi.get(7)\n",
        "\n",
        "                elif(self.raise_r1==0 and action1 + action2 == 'raiseraise'):\n",
        "                    action_num = self.player1.pi.get(7)\n",
        "\n",
        "                elif(self.raise_r1==1 and action1 + action2 == 'raiseraise'):\n",
        "                    action_num = self.player1.pi.get(8)\n",
        "\n",
        "                elif(self.raise_r1==2 and action1 + action2 == 'checkraise'):\n",
        "                    action_num = self.player1.pi.get(8)\n",
        "\n",
        "                elif(self.raise_r1==2 and action1 + action2 == 'raiseraise'):\n",
        "                    action_num = self.player1.pi.get(9)\n",
        "\n",
        "                #print(action_num)\n",
        "                action3 = self.actions_dic.get(action_num)\n",
        "            else:\n",
        "                action3 = self.player1.step([self.public_cards, legal_actions])\n",
        "\n",
        "            if action3 == 'call':\n",
        "                if(self.q1 == True and self.player1.q_learn == True):\n",
        "                    self.rewards.append(0)\n",
        "                self.player1.in_chips += self.bet\n",
        "                if self.feedback:\n",
        "                    print(\"Player 1 has called.\")\n",
        "            elif action3 == 'fold':\n",
        "                self.player1.status = 'folded'\n",
        "                self.player2.win_chips = self.player1.in_chips\n",
        "                self.player1.win_chips = -self.player1.in_chips\n",
        "                if(self.q1 == True and self.player1.q_learn == True):\n",
        "                    self.rewards.append(self.player1.win_chips)\n",
        "                if self.feedback:\n",
        "                    print(\"Player 1 has folded.\")\n",
        "                break\n",
        "            else:\n",
        "                #print(\"3rd raise\")\n",
        "                exit(0)\n",
        "\n",
        "        if i == 0:\n",
        "            self.public_cards = pre_public_cards\n",
        "            self.raise_r1 = raise_count\n",
        "\n",
        "    #Judge and print winners and winchips\n",
        "    players = [self.player1, self.player2]\n",
        "\n",
        "\n",
        "\n",
        "    if self.feedback:\n",
        "        print(\"PLayer 1 has played \"+str(self.player1.in_chips) + \" chips, Player 2 has played: \"+ str(self.player2.in_chips) + \" chips\")\n",
        "        print('Hands'+\" \"+self.player1.hand.rank+\" \"+ self.player2.hand.rank)\n",
        "\n",
        "\n",
        "    #print(\"Ranks: \"+ str(players[0].hand.rank) + ' '+ str(players[1].hand.rank))\n",
        "    if (self.player1.win_chips == 0 and self.player2.win_chips == 0):\n",
        "        if self.feedback:\n",
        "            print(self.public_cards[0].rank, self.public_cards[1].rank)\n",
        "        if None in self.public_cards:\n",
        "            print(\"Xasame\")\n",
        "        winnings = self.judger.judge_game(players, self.public_cards)\n",
        "        self.player1.win_chips = winnings[0]\n",
        "        self.player2.win_chips = winnings[1]\n",
        "\n",
        "      ########## Q L E A R N I N G ############\n",
        "    if(self.q1 == True and self.player1.status == 'alive'):\n",
        "       state = (self.player1.hand, self.public_cards, self.rounds+1, legal_actions)\n",
        "       self.states.append(state)\n",
        "       action_terminate = None\n",
        "       self.actions.append(action_terminate)\n",
        "       self.rewards.append(self.player1.win_chips)\n",
        "    if(self.q2 == True and self.player1.status == 'alive'):\n",
        "      state = (self.player2.hand, self.public_cards, self.rounds+1, legal_actions)\n",
        "      action_terminate = None\n",
        "      self.actions.append(action_terminate)\n",
        "      self.states.append(state)\n",
        "      self.rewards.append(self.player2.win_chips)\n",
        "\n",
        "    if ( self.player1.win_chips > self.player2.win_chips and self.feedback):\n",
        "        print(\"Player 1 won: \"+ str(self.player1.win_chips))\n",
        "    elif ( self.player1.win_chips < self.player2.win_chips and self.feedback):\n",
        "        print(\"Player 2 won: \"+ str(self.player2.win_chips))\n",
        "    else:\n",
        "        if self.feedback:\n",
        "            print(\"Pot split in half: \"+ str(self.player2.win_chips))\n",
        "    if self.feedback:\n",
        "        print(self.player1.win_chips, self.player2.win_chips)\n",
        "    return self.player1.win_chips, self.player2.win_chips\n",
        "\n",
        "\n",
        "########## Q L E A R N I N G ############\n",
        "class QLearningAgent:\n",
        "    def __init__(self, q_learn=False, alpha=0.5, gamma=0.9, epsilon=0.1):\n",
        "        self.alpha = alpha  # learning rate\n",
        "        self.gamma = gamma  # discount factor\n",
        "        self.epsilon = epsilon  # exploration rate\n",
        "        self.q_table = {}  # Q-table\n",
        "        self.q_learn = q_learn\n",
        "\n",
        "    def get_state_key(self, state):\n",
        "        # Convert the state to a unique string key for the Q-table\n",
        "        #if state is None:\n",
        "        #  return \"default_key\"\n",
        "        hand_key = state[0].rank\n",
        "        if None in state[1]:\n",
        "            public_cards_key = \"-\".join(str([hand for hand in state[1]]))\n",
        "        else:\n",
        "            public_cards_key = \"-\".join([hand.rank for hand in state[1]])\n",
        "        round_key = str(state[2])\n",
        "        legal_actions_key = \"-\".join(str([legal for legal in state[3]]))\n",
        "        return f\"{hand_key}_{public_cards_key}_{round_key}_{legal_actions_key}\"\n",
        "\n",
        "\n",
        "    #from current state return the action that corresponds to the highest q_value\n",
        "    def get_best_action(self, state):\n",
        "        actions_dic = {0: \"call\", 1: \"fold\", 2: \"check\", 3: \"raise\"}\n",
        "        state_key = self.get_state_key(state)\n",
        "        legal_actions = state[3]\n",
        "        q_values = self.q_table.get(state_key, np.zeros(4))\n",
        "        max_q_value = np.max(q_values)\n",
        "        best_actions = [action for action, q_value in enumerate(q_values) if q_value == max_q_value and legal_actions[action] == 1]\n",
        "        best_actions_str = []\n",
        "        for action_num in best_actions:\n",
        "            best_actions_str.append(actions_dic.get(action_num))\n",
        "        if (best_actions_str == []):\n",
        "            print('s')\n",
        "        return np.random.choice(best_actions_str)\n",
        "\n",
        "    #update the q_table with the new q values\n",
        "    def update_q_table(self, state, action, next_state, reward):\n",
        "        state_key = self.get_state_key(state)\n",
        "        if next_state == None:\n",
        "            q_values = self.q_table.get(state_key, np.zeros(4))\n",
        "            reverse_actions_dic = {\"call\": 0, \"fold\":1 , \"check\":2 , \"raise\":3 }\n",
        "            action = reverse_actions_dic.get(action)\n",
        "            q_values[action] += self.alpha * (reward - q_values[action])\n",
        "            for index, value in enumerate(state[3]):\n",
        "                if value == 0:\n",
        "                    q_values[index] = -1000\n",
        "                else:\n",
        "                    continue\n",
        "            self.q_table[state_key] = q_values\n",
        "        else:\n",
        "            next_state_key = self.get_state_key(next_state)\n",
        "            q_values = self.q_table.get(state_key, np.zeros(4))\n",
        "            next_q_values = self.q_table.get(next_state_key, np.zeros(4))\n",
        "            reverse_actions_dic = {\"call\": 0, \"fold\":1 , \"check\":2 , \"raise\":3 }\n",
        "            action = reverse_actions_dic.get(action)\n",
        "            q_values[action] += self.alpha * (reward + self.gamma * np.max(next_q_values) - q_values[action])\n",
        "            for index, value in enumerate(state[3]):\n",
        "                if value == 0:\n",
        "                    q_values[index] = -1000\n",
        "                else:\n",
        "                    continue\n",
        "            self.q_table[state_key] = q_values\n",
        "\n",
        "    #epsilon defines the exploration.\n",
        "    #While training there is a chance of exploring or choosing the action with the best q_value\n",
        "    def step_train(self, state):\n",
        "        #print(state)\n",
        "        legal_actions = state[3]\n",
        "        actions_dic = {0: \"call\", 1: \"fold\", 2: \"check\", 3: \"raise\"}\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            action_num =  np.random.choice([action for action, legal in enumerate(legal_actions) if legal == 1])\n",
        "            action = actions_dic.get(action_num)\n",
        "            return action\n",
        "        else:\n",
        "            return self.get_best_action(state)\n",
        "\n",
        "    def step_test(self, state):\n",
        "        #print(state)\n",
        "        return self.get_best_action(state)\n",
        "\n",
        "\n",
        "#A policy agent class\n",
        "class PolicyAgent:\n",
        "  #Initialize with player id(1 or 2) and opponent type(random or threshold)\n",
        "  def __init__(self, ID, opp, gamma = 0.9, epsilon = 0.0000001):\n",
        "        self.in_chips = 0\n",
        "        self.win_chips = 0\n",
        "        self.hand = None\n",
        "        self.status = 'alive'\n",
        "        self.ID = ID\n",
        "        self.opponent = opp\n",
        "        self.pi = None\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "  #Find the public card with the highest rank\n",
        "  def max_public(self, public_cards):\n",
        "        ranks = []\n",
        "        for i in public_cards:\n",
        "          ranks.append(rank_int(i.rank))\n",
        "        if ranks[0] > ranks[1]:\n",
        "          return public_cards[0]\n",
        "        else:\n",
        "          return public_cards[1]\n",
        "\n",
        "  #Find the probability of Threshold having one pair or three of a kind but no A or K\n",
        "  def pair_probability(self, public_cards):\n",
        "        #if you dont have pair or 3 of a kind\n",
        "        if self.hand not in public_cards:\n",
        "            if(public_cards[0].rank == public_cards[1].rank):\n",
        "                return 2/17\n",
        "            else:\n",
        "                return 6/17\n",
        "        #if you have pair or 3 of a kind but also not A or K\n",
        "        elif (self.hand.rank != 'A' or self.hand.rank != 'K') and self.hand in public_cards:\n",
        "            if(public_cards[0].rank == public_cards[1].rank):\n",
        "                return 1/17\n",
        "            else:\n",
        "                return 5/17\n",
        "        elif (self.hand.rank == 'A' or self.hand.rank == 'K') and self.hand in public_cards:\n",
        "            #if public cards == AA or KK or AK or KA then Thershold does not have pair\n",
        "            if(public_cards[0].rank == public_cards[1].rank or ('K' in [public_cards[0].rank, public_cards[1].rank] and 'A' in [public_cards[0].rank, public_cards[1].rank])):\n",
        "                return 0\n",
        "            else:\n",
        "                return 3/17\n",
        "\n",
        "  #Find the probability of Threshold having one pair or three of a kind with A or K\n",
        "  def pair_probability_high(self,public_cards):\n",
        "      #if public cards are AA or KK\n",
        "      if (public_cards[0] == public_cards[1] and (public_cards[0].rank == 'A' or public_cards[0].rank == 'K')):\n",
        "          return 0.5\n",
        "      #if 2 different public cards down and only one of them is A or K\n",
        "      elif ((public_cards[0].rank == 'A' or public_cards[0].rank == 'K') and (public_cards[1].rank != 'A' or public_cards[1].rank != 'K')):\n",
        "          return 0.5\n",
        "      #if public cards are AK or KA\n",
        "      elif ((public_cards[0].rank == 'A' or public_cards[0].rank == 'K') and (public_cards[1].rank == 'A' or public_cards[1].rank == 'K')):\n",
        "          return 1\n",
        "      else:\n",
        "          return 0\n",
        "\n",
        "\n",
        "  #Find probability in Round 2 of PolicyAgent  winning if Threshold has no A,K or pair\n",
        "  def no_pair_high(self, public_cards):\n",
        "      if self.hand.rank == 'Q' or self.hand in public_cards:\n",
        "          w_prob = 1\n",
        "      elif self.hand.rank == 'J':\n",
        "          w_prob = 13/17\n",
        "      else :\n",
        "          w_prob = 9/17\n",
        "      return w_prob\n",
        "\n",
        "  #Find probability in Round 2 of PolicyAgent winning if Threshold has not A,K but has pair\n",
        "  def pair_no_high(self, public_cards):\n",
        "      if(self.max_public(public_cards).rank == self.hand.rank):\n",
        "           w_prob = 1\n",
        "      elif((self.hand.rank == 'K' or self.hand.rank == 'Q' ) and self.hand in public_cards):\n",
        "           w_prob = 1\n",
        "      elif(self.hand in public_cards and public_cards[0] == public_cards[1]):\n",
        "           w_prob = 1\n",
        "      elif(self.hand.rank == 'J' and self.hand in public_cards):\n",
        "           w_prob = 14/17\n",
        "      elif(self.hand.rank == 'T' and self.hand in public_cards):\n",
        "           w_prob = 11/17\n",
        "      else:\n",
        "          w_prob = 0\n",
        "      return w_prob\n",
        "\n",
        "  #Find probability in Round 2 of PolicyAgent winning if Threshold has A,K but no pair\n",
        "  def high_no_pair(self, public_cards):\n",
        "        if(self.hand in public_cards or self.hand.rank == 'A'):\n",
        "            w_prob = 1\n",
        "        elif(self.hand.rank == 'K'):\n",
        "            w_prob = 14/17\n",
        "        else:\n",
        "            w_prob = 0\n",
        "\n",
        "        return w_prob\n",
        "\n",
        "  #Find probability in Round 2 of PolicyAgent winning if Threshold has AA or KK\n",
        "  def high_pair(self,public_cards):\n",
        "      if(self.max_public(public_cards).rank == self.hand.rank):\n",
        "          w_prob = 1\n",
        "      else:\n",
        "          w_prob = 0\n",
        "      return w_prob\n",
        "\n",
        "  #Find probability in Round 1 of  Threshold having A or K\n",
        "  def has_high(self):\n",
        "      if(self.hand == 'A' or self.hand == 'K'):\n",
        "          high_prob = 7/19\n",
        "      else:\n",
        "          high_prob = 8/19\n",
        "      return high_prob\n",
        "\n",
        "\n",
        "  #Compute the probabilities of winning and losing against random opponent\n",
        "  def evaluate_vs_Random(self, public_cards):\n",
        "        #Round 1\n",
        "        if public_cards == [None, None] and self.hand.rank == 'A':\n",
        "            w_prob = 1\n",
        "            l_prob = 0\n",
        "        elif public_cards == [None, None] and self.hand.rank == 'K':\n",
        "            w_prob = 4/5\n",
        "            l_prob = 1/5\n",
        "        elif public_cards == [None, None] and self.hand.rank == 'Q':\n",
        "            w_prob = 3/5\n",
        "            l_prob = 2/5\n",
        "        elif public_cards == [None, None] and self.hand.rank == 'J':\n",
        "            w_prob = 2/5\n",
        "            l_prob = 3/5\n",
        "        elif public_cards == [None, None] and self.hand.rank == 'T':\n",
        "            w_prob = 1/5\n",
        "            l_prob = 4/5\n",
        "\n",
        "        #Round 2\n",
        "        #2 same public cards\n",
        "        if public_cards[0].rank == public_cards[1].rank:\n",
        "            if self.hand.rank == public_cards[0].rank:\n",
        "                w_prob = 1\n",
        "                l_prob = 0\n",
        "            elif self.hand.rank == 'A':\n",
        "                w_prob = 15/17\n",
        "                l_prob = 2/17\n",
        "            elif self.hand.rank == 'K':\n",
        "                #Other player wins with 3 of a kind or A\n",
        "                w_prob = 11/17\n",
        "                l_prob = 6/17\n",
        "            elif self.hand.rank == 'Q':\n",
        "                if rank_int(public_cards[0].rank) < 3:\n",
        "                    w_prob = 7/17\n",
        "                    l_prob = 10/17\n",
        "                else:\n",
        "                    w_prob = 11/17\n",
        "                    l_prob = 6/17\n",
        "            elif self.hand.rank == 'J':\n",
        "            #Other player wins with 3 of a kind or A, K, Q\n",
        "            #If 2 10's are dealt then you lose if other player has one of 4 remaining A's | K's | Q's\n",
        "                if public_cards[0].rank == 'T':\n",
        "                    w_prob = 3/17\n",
        "                    l_prob = 14/17\n",
        "            #If other player doesnt have 3 of a kind but 2 A's | K's .... are dealt, he has one of 4 remaining A's | K's | Q's (depending which of them are not dealt)\n",
        "                else:\n",
        "                    w_prob = 7/17\n",
        "                    l_prob = 10/17\n",
        "            else:\n",
        "            #Other player wins with 3 of a kind or A, K, Q, J\n",
        "                w_prob = 3/17\n",
        "                l_prob = 14/17\n",
        "\n",
        "        #2 different public cards\n",
        "        else:\n",
        "            #If player matches one of the public cards(pair)\n",
        "            if self.hand.rank == public_cards[0].rank or self.hand.rank == public_cards[1].rank:\n",
        "                #If he matches the public card with the highest rank, the player wins\n",
        "                if self.hand.rank == self.max_public(public_cards):\n",
        "                    w_prob = 1\n",
        "                    l_prob = 0\n",
        "                #Else, the other player could match the highest public card and win\n",
        "                else:\n",
        "                    w_prob = 14/17\n",
        "                    l_prob = 3/17\n",
        "            #Else, with no pair or 3 of a kind:\n",
        "            #If the player has A, he wins if the other player doesn't have a pair\n",
        "            elif self.hand.rank == 'A':\n",
        "                w_prob = 11/17\n",
        "                l_prob = 6/17\n",
        "            #If the player has K, he wins if the other player doesn't have a pair or A\n",
        "            elif self.hand.rank == 'K':\n",
        "                w_prob = 7/17\n",
        "                l_prob = 10/17\n",
        "            #If the player has Q, he wins if the other player doesn't have a pair or A or K\n",
        "            elif self.hand.rank == 'Q':\n",
        "                #If J and 10 are the public cards, then the other player can have A or K with probability 8/17\n",
        "                if (public_cards[0].rank == 'J' and public_cards[1].rank == 'T') or (public_cards[0].rank == 'T' and public_cards[1].rank == 'J'):\n",
        "                    w_prob = 3/17\n",
        "                    l_prob = 14/17\n",
        "                #Else, the other player can have A or K with probability 4/17 (A or K are in public cards but the other player doesn't have pair)\n",
        "                else:\n",
        "                    w_prob = 7/17\n",
        "                    l_prob = 10/17\n",
        "            #If the player has J, he wins if the other player has 10 or J\n",
        "            elif self.hand.rank == 'J':\n",
        "                if public_cards[0].rank == 'T' or  public_cards[1].rank == 'T':\n",
        "                    w_prob = 3/17\n",
        "                    l_prob = 14/17\n",
        "                else:\n",
        "                    w_prob = 7/17\n",
        "                    l_prob = 10/17\n",
        "            #If the player has 10, he wins only if the other player has 10\n",
        "            else:\n",
        "                w_prob = 3/17\n",
        "                l_prob = 14/17\n",
        "\n",
        "        return w_prob, l_prob\n",
        "\n",
        "  def get_P(self, penalty, public_cards):\n",
        "     w_prob2, l_prob2 = self.evaluate_vs_Random(public_cards)\n",
        "     #print(w_prob2)\n",
        "     #print(l_prob2)\n",
        "     pair_prob = self.pair_probability(public_cards)\n",
        "     pair_prob_high = self.pair_probability_high(public_cards)\n",
        "     w_prob_no_pair_high = self.no_pair_high(public_cards)\n",
        "     w_prob_pair_no_high = self.pair_no_high(public_cards)\n",
        "     w_prob_high_no_pair = self.high_no_pair(public_cards)\n",
        "     w_prob_high_pair = self.high_pair(public_cards)\n",
        "     prob_high = self.has_high()\n",
        "\n",
        "     #If agent is player 2 against random opponent\n",
        "     if self.ID == 2 and self.opponent == 'Random':\n",
        "            P = {\n",
        "            # Round 1\n",
        "            #We Suppose that Agent is Player 2\n",
        "            #Player 1 has checked (possible initial state)\n",
        "            0: {\n",
        "\n",
        "                #Call (Illegal)\n",
        "                0: [(1, 0, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Fold (Illegal)\n",
        "                1: [(1, 0, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Check , Go to round 2\n",
        "                2: [(0.5, 2, 0, False), #go round 2 => 0 raise (2 initial states)\n",
        "                    (0.5, 3, 0, False)  #go round 2 => 0 raise (2 initial states)\n",
        "\n",
        "                ],\n",
        "                #Raise\n",
        "                3: [(0.25, 4, 0, False), #check raise call go round 2 => 1 raise (2 initial states)\n",
        "                    (0.25, 5, 0, False), #check raise call go round 2 => 1 raise (2 initial states)\n",
        "                    (0.5, 9, 0.5, True)   #check raise fold\n",
        "                ]\n",
        "            },\n",
        "            #Player 1 has raised (possible initial state)\n",
        "            1: {\n",
        "                #Call\n",
        "                0: [(0.5, 4, 0, False), #raise call go round 2 => 1 raise (2 initial states)\n",
        "                    (0.5, 5, 0, False)  #raise call go round 2 => 1 raise (2 initial states)\n",
        "                ],\n",
        "                #Fold\n",
        "                1: [(1, 8, -0.5, True) #raise fold\n",
        "\n",
        "                ],\n",
        "\n",
        "                #Check (Illegal)\n",
        "                2: [(1, 1, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Raise\n",
        "                3: [(0.25, 6, 0, False), #raise raise call 2 raises (2 initial states)\n",
        "                    (0.25, 7, 0, False), #raise raise call 2 raises (2 initial states)\n",
        "                    (0.5, 9, 1.5, True)  #raise raise fold\n",
        "                ]\n",
        "            },\n",
        "            # Round 2 after: 0 raises\n",
        "            #Player 1 has checked (possible initial state)\n",
        "            2: {\n",
        "\n",
        "                #Call (Illegal)\n",
        "                0: [(1, 2, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Fold (Illegal)\n",
        "                1: [(1, 2, penalty, False) #remain to same state\n",
        "\n",
        "                ],\n",
        "                #Check\n",
        "                2: [(w_prob2, 9, 0.5, True), #check check\n",
        "                    (l_prob2, 8, -0.5, True)\n",
        "                ],\n",
        "                #Raise\n",
        "                3: [((0.5)*w_prob2, 9, 1.5, True), #check raise call , 0 + 1 raises\n",
        "                    ((0.5)*l_prob2, 8, -1.5, True), #check raise call,  0 + 1 raises\n",
        "                    (0.5, 9, 0.5, True)   #check raise fold\n",
        "                ]\n",
        "            },\n",
        "            # Round 2 after: 0 raises\n",
        "            #Player 1 has raised (possible initial state)\n",
        "            3: {\n",
        "                #Call\n",
        "                0: [(w_prob2, 9, 1.5, True),\n",
        "                    (l_prob2, 8, -1.5, True)\n",
        "                ],\n",
        "                #Fold\n",
        "                1: [(1, 8, -1.5, True)\n",
        "                ],\n",
        "\n",
        "                #Check (Illegal)\n",
        "                2: [(1, 3, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Raise\n",
        "                3: [(0.5*w_prob2, 9, 2.5, True), #raise raise call,  0 + 2 raises\n",
        "                    (0.5*l_prob2, 8, -2.5, True),  #raise raise call,  0 + 2 raises\n",
        "                    (0.5, 9, 2.5, True)   #raise raise fold\n",
        "                ]\n",
        "            },\n",
        "            # Round 2 after: 1 raise  4,5 reserved\n",
        "            #Player 1 has checked (possible initial state)\n",
        "            4: {\n",
        "                #Call (Illegal)\n",
        "                0: [(1, 4, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Fold (Illegal)\n",
        "                1: [(1, 4, penalty, False) #remain to same state\n",
        "                ],\n",
        "                #Check\n",
        "                2: [(w_prob2, 9, 1.5, True), #check check\n",
        "                    (l_prob2, 8, -1.5, True)\n",
        "                ],\n",
        "                #Raise\n",
        "                3: [((0.5)*w_prob2, 9, 2.5, True), #check raise call , 0 + 1 raises\n",
        "                    ((0.5)*l_prob2, 8, -2.5, True), #check raise call,  0 + 1 raises\n",
        "                    (0.5, 9, 1.5, True)   #check raise fold\n",
        "                ]\n",
        "            },\n",
        "            # Round 2 after: 1 raises\n",
        "            #Player 1 has raised (possible initial state)\n",
        "            5: {\n",
        "                #Call\n",
        "                0: [(w_prob2, 9, 2.5, True),\n",
        "                    (l_prob2, 8, -2.5, True)\n",
        "                ],\n",
        "                #Fold\n",
        "                1: [(1, 8, -2.5, True)\n",
        "                ],\n",
        "\n",
        "                #Check (Illegal)\n",
        "                2: [(1, 5, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Raise\n",
        "                3: [(0.5*w_prob2, 9, 3.5, True), #raise raise call,  0 + 2 raises\n",
        "                    (0.5*l_prob2, 8, -3.5, True),  #raise raise call,  0 + 2 raises\n",
        "                    (0.5, 9, 3.5, True)   #raise raise fold\n",
        "                ]\n",
        "            },\n",
        "\n",
        "            # Round 2 after: 2 raises  6,7 reserved\n",
        "            #Player 1 has checked (possible initial state)\n",
        "            6: {\n",
        "                #Call (Illegal)\n",
        "                0: [(1, 6, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Fold (Illegal)\n",
        "                1: [(1, 6, penalty, False) #remain to same state\n",
        "\n",
        "                ],\n",
        "                #Check\n",
        "                2: [(w_prob2, 9, 2.5, True), #check check\n",
        "                    (l_prob2, 8, -2.5, True)\n",
        "                ],\n",
        "                #Raise\n",
        "                3: [((0.5)*w_prob2, 9, 3.5, True), #check raise call , 0 + 1 raises\n",
        "                    ((0.5)*l_prob2, 8, -3.5, True), #check raise call,  0 + 1 raises\n",
        "                    (0.5, 9, 2.5, True)   #check raise fold\n",
        "                ]\n",
        "            },\n",
        "            # Round 2 after: 2 raises\n",
        "            #Player 1 has raised (possible initial state)\n",
        "            7: {\n",
        "                #Call\n",
        "                0: [(w_prob2, 9, 3.5, True),\n",
        "                    (l_prob2, 8, -3.5, True)\n",
        "                ],\n",
        "                #Fold\n",
        "                1: [(1, 8, -3.5, True)\n",
        "                ],\n",
        "\n",
        "                #Check (Illegal)\n",
        "                2: [(1, 7, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Raise\n",
        "                3: [(0.5*w_prob2, 9, 4.5, True), #raise raise call,  0 + 2 raises\n",
        "                    (0.5*l_prob2, 8, -4.5, True),  #raise raise call,  0 + 2 raises\n",
        "                    (0.5, 9, 4.5, True)   #raise raise fold\n",
        "                ]\n",
        "            },\n",
        "\n",
        "            #For Loss\n",
        "            8: {\n",
        "                0: [(1.0, 8, 0, True)],\n",
        "                1: [(1.0, 8, 0, True)],\n",
        "                2: [(1.0, 8, 0, True)],\n",
        "                3: [(1.0, 8, 0, True)]\n",
        "            },\n",
        "            #For Win\n",
        "            9: {\n",
        "                0: [(1.0, 9, 0, True)],\n",
        "                1: [(1.0, 9, 0, True)],\n",
        "                2: [(1.0, 9, 0, True)],\n",
        "                3: [(1.0, 9, 0, True)]\n",
        "            }\n",
        "            }\n",
        "\n",
        "     #Î™f agent is player 1 against random opponent\n",
        "     elif self.ID== 1 and self.opponent == 'Random':\n",
        "        P = {\n",
        "            #Round 1: (initial state)\n",
        "            0: {\n",
        "                #Call (Illegal)\n",
        "                0: [(1, 0, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Fold (Illegal)\n",
        "                1: [(1, 0, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Check\n",
        "                2: [(0.33, 3, 0, False), # check check\n",
        "                    (0.33, 1, 0, False), # check raise\n",
        "                    (0.33, 10, 0, True)  # check fold\n",
        "                ],\n",
        "\n",
        "                #Raise\n",
        "                3: [(0.33, 4, 0, False), #raise call\n",
        "                    (0.33, 2, 0, False), #raise raise\n",
        "                    (0.33, 10, 0.5, True) #raise fold\n",
        "                ]\n",
        "           },\n",
        "            #Round 1: 1 raise\n",
        "            1: {\n",
        "                #Call\n",
        "                0: [(1, 4, 0, False), #check raise call go to round 2 => 1 raise\n",
        "                ],\n",
        "\n",
        "                #Fold\n",
        "                1: [(1, 11, -0.5, True), #check raise fold\n",
        "                ],\n",
        "\n",
        "                #Check (Illegal)\n",
        "                2: [(1, 1, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Raise (Illegal)\n",
        "                3: [(1, 1, penalty, False) #remain to same state\n",
        "                ]\n",
        "            },\n",
        "            #Round 1: raise raise\n",
        "            2: {\n",
        "                #Call\n",
        "                0: [(1, 5, 0.0, False), #raise raise call go Round 2 => 2 raises\n",
        "                ],\n",
        "\n",
        "                #Fold\n",
        "                1: [(1, 11, -1.5, True), # raise raise fold\n",
        "                ],\n",
        "\n",
        "                #Check (Illegal)\n",
        "                2: [(1, 2, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Raise (Illegal)\n",
        "                3: [(1, 2, penalty, False) #remain to same state\n",
        "                ]\n",
        "            },\n",
        "            #Round 2 (initial state) 0 raises\n",
        "            #0 raises\n",
        "            3: {\n",
        "                #Call (illegal)\n",
        "                0: [(1, 3, penalty, False), #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Fold (illegal)\n",
        "                1: [(1, 3, penalty, False), #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Check\n",
        "                2: [(0.33*w_prob2, 10, 0.5, True), #check check (win)\n",
        "                    (0.33*l_prob2, 11, -0.5, True), #check check (lose)\n",
        "                    (0.33, 10, 0.5, True), #check fold\n",
        "                    (0.33, 6, 0.0, False) #check raise\n",
        "                ],\n",
        "\n",
        "                #Raise\n",
        "                3: [(0.33*w_prob2, 10, 1.5 , True), #raise call (win)\n",
        "                    (0.33*l_prob2, 11, -1.5 , True), #raise call (lose)\n",
        "                    (0.33, 7, 0.0, False), #raise raise\n",
        "                    (0.33, 10, 0.5, True) #raise fold\n",
        "                ]\n",
        "            },\n",
        "            #Round 2 (initial state) 1 raise\n",
        "            4: {\n",
        "                #Call (illegal)\n",
        "                0: [(1, 4, penalty, False), #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Fold (illegal)\n",
        "                1: [(1, 4, penalty, False), #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Check\n",
        "                2: [((w_prob2)*0.5, 10, 1.5, True), #check check (win)\n",
        "                    ((l_prob2)*0.5, 11, -1.5, True), #check check (lose)\n",
        "                    (0.5, 7, 0.0, False) #check raise (2 raises)\n",
        "                ],\n",
        "\n",
        "                #Raise\n",
        "                3: [(0.33*(w_prob2), 10, 2.5 , True), #raise call (win)\n",
        "                    (0.33*(l_prob2), 11, -2.5 , True), #raise call (lose)\n",
        "                    (0.33, 8, 0, False), #raise raise\n",
        "                    (0.33, 10, 1.5, True) #raise fold\n",
        "                ]\n",
        "            },\n",
        "            #Round 2 (initial state) 2 raises\n",
        "            5: {\n",
        "                #Call (illegal)\n",
        "                0: [(1, 5, penalty, False), #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Fold (illegal)\n",
        "                1: [(1, 5, penalty, False), #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Check\n",
        "                2: [(0.5*w_prob2, 10, 2.5, True), #check check (win)\n",
        "                    (0.5*l_prob2, 11, -2.5, True), #check check (lose)\n",
        "                    (0.5, 8, 0.0, False) #check raise\n",
        "                ],\n",
        "\n",
        "                #Raise\n",
        "                3: [(0.33*w_prob2, 10, 3.5 , True), #raise call (win)\n",
        "                    (0.33*l_prob2, 11, -3.5 , True), #raise call (lose)\n",
        "                    (0.33, 9, 0.0, False), #raise raise\n",
        "                    (0.33, 10, 2.5, True) #raise fold\n",
        "                ]\n",
        "            },\n",
        "            #Check raise (0 raises from Round 1)\n",
        "            6: {\n",
        "                #Call\n",
        "                0: [(w_prob2, 10, 1.5, True), #check raise call (win)\n",
        "                    (l_prob2, 11, -1.5, True) #check raise call (lose)\n",
        "                ],\n",
        "\n",
        "                #Fold\n",
        "                1: [(1, 11, -0.5, True), #check raise fold\n",
        "                ],\n",
        "\n",
        "                #Check (Illegal)\n",
        "                2: [(1, 6, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Raise (Illegal)\n",
        "                3: [(1, 6, penalty, False) #remain to same state\n",
        "                ]\n",
        "\n",
        "            },\n",
        "            #Check raise (1 raise from Round 1) or Raise raise (0 raises from round 1)\n",
        "            7: {\n",
        "                #call\n",
        "                0: [(w_prob2, 10, 2.5, True), #check raise call (win)\n",
        "                    (l_prob2, 11, -2.5, True) #check raise call (lose)\n",
        "                ],\n",
        "                #fold\n",
        "                1: [(1, 11, -1.5, True), #check raise fold\n",
        "                ],\n",
        "                #Check (Illegal)\n",
        "                2: [(1, 7, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Raise (Illegal)\n",
        "                3: [(1, 7, penalty, False) #remain to same state\n",
        "                ]\n",
        "                },\n",
        "            #raise raise (1 raise from round 1) or check raise (2 raises from round 1)\n",
        "            8: {\n",
        "                #Call\n",
        "                0: [(w_prob2, 10, 3.5, True), #raise raise call (win)\n",
        "                    (l_prob2, 11, -3.5, True) #raise raise call (lose)\n",
        "                ],\n",
        "\n",
        "                #Fold\n",
        "                1: [(1, 11, -2.5, True), #raise raise fold\n",
        "                ],\n",
        "\n",
        "                #Check (Illegal)\n",
        "                2: [(1, 8, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Raise (Illegal)\n",
        "                3: [(1, 8, penalty, False) #remain to same state\n",
        "                ]\n",
        "\n",
        "            },\n",
        "            #Raise raise (2 raises from Round 1)\n",
        "            9: {\n",
        "                #Call\n",
        "                0: [(w_prob2, 10, 4.5, True), #raise raise call (win)\n",
        "                    (l_prob2, 11, -4.5, True) #raise raise call (lose)\n",
        "                ],\n",
        "\n",
        "                #Fold\n",
        "                1: [(1, 11, -3.5, True), #raise raise fold\n",
        "                ],\n",
        "\n",
        "                #Check (Illegal)\n",
        "                2: [(1, 9, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Raise (Illegal)\n",
        "                3: [(1, 9, penalty, False) #remain to same state\n",
        "                ]\n",
        "            },\n",
        "\n",
        "            #Winning\n",
        "            10: {\n",
        "                  0: [(1, 10, 0, True)],\n",
        "                  1: [(1, 10, 0, True)],\n",
        "                  2: [(1, 10, 0, True)],\n",
        "                  3: [(1, 10, 0, True)]\n",
        "            },\n",
        "\n",
        "            #Losing\n",
        "            11: {\n",
        "                  0: [(1, 11, 0, True)],\n",
        "                  1: [(1, 11, 0, True)],\n",
        "                  2: [(1, 11, 0, True)],\n",
        "                  3: [(1, 11, 0, True)]\n",
        "            }\n",
        "        }\n",
        "\n",
        "     #If agent is player 2 against threshold opponent\n",
        "     elif self.ID == 2 and self.opponent == 'Threshold':\n",
        "            P = {\n",
        "            # Round 1\n",
        "            #Player 1 has checked (possible initial state)             #####3/5 i pithanotita na jekinisei me 0 initial state\n",
        "            0: {\n",
        "                #Call (Illegal)\n",
        "                0: [(1, 0, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Fold (Illegal)\n",
        "                1: [(1, 0, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Check , Go to round 2\n",
        "                2: [(1-pair_prob, 2, 0, False), #go round 2 => 0 raise (2 initial states) check check\n",
        "                    (pair_prob, 3, 0, False)  #go round 2 => 0 raise (2 initial states) check check\n",
        "                ],\n",
        "\n",
        "                #Raise\n",
        "                3: [(1, 9, 0.5, True)   #check raise fold       #can't call because he doesn't have K or A\n",
        "                ]\n",
        "            },\n",
        "            #Player 1 has raised (possible initial state)\n",
        "            1: {\n",
        "                #Call\n",
        "                 0: [(1-pair_prob_high, 4, 0, False), #raise call go round 2 => 1 raise (2 initial states)\n",
        "                    (pair_prob_high, 5, 0, False)  #raise call go round 2 => 1 raise (2 initial states)\n",
        "                ],\n",
        "\n",
        "                #Fold\n",
        "                1: [(1, 9, -0.5, True) #raise fold\n",
        "                ],\n",
        "\n",
        "                #Check (Illegal)\n",
        "                2: [(1, 1, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Raise\n",
        "                3: [(1-pair_prob, 6, 0, False), #raise raise call 2 raises (2 initial states)     #can't fold because he has K or A\n",
        "                    (pair_prob, 7, 0, False) #raise raise call 2 raises (2 initial states)\n",
        "                ]\n",
        "            },\n",
        "            # Round 2 after: 0 raises\n",
        "            #Player 1 has checked (possible initial state) Player 1 has no A or K and no pair\n",
        "            2: {\n",
        "                #Call (Illegal)\n",
        "                0: [(1, 2, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Fold (Illegal)\n",
        "                1: [(1, 2, penalty, True)\n",
        "                ],\n",
        "\n",
        "                #Check\n",
        "                2: [(w_prob_no_pair_high, 9, 0.5, True), #check check\n",
        "                    (1-w_prob_no_pair_high, 8, -0.5, True)\n",
        "                ],\n",
        "\n",
        "                #Raise\n",
        "                3: [(1, 9, 0.5, True)   #check raise fold        #can't call because he doesn't have a pair\n",
        "                ]\n",
        "            },\n",
        "            # Round 2 after: 0 raises\n",
        "            #Player 1 has raised (possible initial state) Player 1 has not A or K but has pair\n",
        "            3: {\n",
        "                #Call\n",
        "                0: [(w_prob_pair_no_high, 9, 1.5, True), #raise call win\n",
        "                    (1-w_prob_pair_no_high, 8, -1.5, True) #raise call lose\n",
        "                ],\n",
        "\n",
        "                #Fold\n",
        "                1: [(1, 8, -1.5, True)\n",
        "                ],\n",
        "\n",
        "                #Check (Illegal)\n",
        "                2: [(1, 3, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Raise\n",
        "                3: [(w_prob_pair_no_high, 9, 2.5, True), #raise raise call (win),  0 + 2 raises\n",
        "                    (1-w_prob_pair_no_high, 8, -2.5, True)  #raise raise call (lose),  0 + 2 raises\n",
        "\n",
        "                ]\n",
        "            },\n",
        "            # Round 2 after: 1 raise  5,6 reserved\n",
        "            #Player 1 has checked (possible initial state) Player 1 has A or K\n",
        "            4: {\n",
        "                #Call (Illegal)\n",
        "                0: [(1, 4, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Fold (Illegal)\n",
        "                1: [(1, 4, penalty, True)\n",
        "                ],\n",
        "\n",
        "                #Check\n",
        "                2: [(w_prob_high_no_pair, 9, 1.5, True), #check check (win)\n",
        "                    (1-w_prob_high_no_pair, 8, -1.5, True) #check check (lose)\n",
        "                ],\n",
        "\n",
        "                #Raise\n",
        "                3: [(1, 9, 1.5, True)   #check raise fold             #can't call because he doesn't have a pair\n",
        "                ]\n",
        "            },\n",
        "            # Round 2 after: 1 raises\n",
        "            #Player 1 has raised Player 1 has pair that is AA or KK\n",
        "            5: {\n",
        "                #Call\n",
        "                0: [(w_prob_high_pair, 9, 2.5, True),\n",
        "                    (1-w_prob_high_pair, 8, -2.5, True)\n",
        "                ],\n",
        "\n",
        "                #Fold\n",
        "                1: [(1, 8, -1.5, True)\n",
        "                ],\n",
        "\n",
        "                #Check (Illegal)\n",
        "                2: [(1, 5, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Raise\n",
        "                3: [(w_prob_high_pair, 9, 3.5, True),   #raise raise call (win)\n",
        "                    (1-w_prob_high_pair, 8, -3.5, True),   #raise raise call (lose)\n",
        "                ]\n",
        "            },\n",
        "\n",
        "            # Round 2 after: 2 raises  6,7 reserved\n",
        "            #Player 1 has checked (possible initial state) Player 1 has A or K\n",
        "            6: {\n",
        "                #Call (Illegal)\n",
        "                0: [(1, 6, penalty, False) #remain to same state\n",
        "                ],\n",
        "                #Fold (Illegal)\n",
        "                1: [(1, 6, penalty, False) #remain to same state\n",
        "                ],\n",
        "                #Check\n",
        "                2: [(w_prob_high_no_pair, 9, 1.5, True), #check check (win)\n",
        "                    (1-w_prob_high_no_pair, 8, -1.5, True) #check check (lose)\n",
        "                ],\n",
        "\n",
        "                #Raise\n",
        "                3: [(1, 9, 1.5, True)   #check raise fold             #can't call because he doesn't have a pair\n",
        "                ]\n",
        "            },\n",
        "            # Round 2 after: 2 raises\n",
        "            #Player 1 has raised (possible initial state) Player 1 has AA or KK\n",
        "            7: {\n",
        "                #Call\n",
        "                0: [(w_prob_high_pair, 9, 2.5, True),\n",
        "                    (1-w_prob_high_pair, 8, -2.5, True)\n",
        "                ],\n",
        "\n",
        "                #Fold\n",
        "                1: [(1, 8, -1.5, True)\n",
        "                ],\n",
        "\n",
        "                #Check (Illegal)\n",
        "                2: [(1, 5, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Raise\n",
        "                3: [(w_prob_high_pair, 9, 3.5, True),   #raise raise call (win)\n",
        "                    (1-w_prob_high_pair, 8, -3.5, True),   #raise raise call (lose)\n",
        "                ]\n",
        "            },\n",
        "            #For Loss\n",
        "            8: {\n",
        "                0: [(1, 8, 0, True)],\n",
        "                1: [(1, 8, 0, True)],\n",
        "                2: [(1, 8, 0, True)],\n",
        "                3: [(1, 8, 0, True)]\n",
        "            },\n",
        "            #For Win\n",
        "            9: {\n",
        "                0: [(1, 9, 0, True)],\n",
        "                1: [(1, 9, 0, True)],\n",
        "                2: [(1, 9, 0, True)],\n",
        "                3: [(1, 9, 0, True)]\n",
        "        }\n",
        "            }\n",
        "     #Î™f agent is player 1 against random opponent\n",
        "     elif self.ID== 1 and self.opponent == 'Threshold':\n",
        "        P = {\n",
        "            #Round 1: (initial state)\n",
        "            0: {\n",
        "                #Call (Illegal)\n",
        "                0: [(1, 0, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Fold (Illegal)\n",
        "                1: [(1, 0, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Check\n",
        "                2: [(1-prob_high, 3, 0, False), # check check\n",
        "                    (prob_high, 1, 0, False) # check raise\n",
        "                ],\n",
        "\n",
        "                #Raise\n",
        "                3: [(prob_high, 2, 0, False), #raise raise\n",
        "                    (1-prob_high, 11, 0.5, True) #raise fold\n",
        "                ]\n",
        "           },\n",
        "            #Round 1: 1 raise\n",
        "            1: {\n",
        "                #Call\n",
        "                0: [(1, 4, 0, False), #check raise call go to round 2 => 1 raise\n",
        "                ],\n",
        "\n",
        "                #Fold\n",
        "                1: [(1, 12, -0.5, True), #check raise fold\n",
        "                ],\n",
        "\n",
        "                #Check (Illegal)\n",
        "                2: [(1, 1, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Raise (Illegal)\n",
        "                3: [(1, 1, penalty, False) #remain to same state\n",
        "                ]\n",
        "            },\n",
        "            #Round 1: raise raise\n",
        "            2: {\n",
        "                #Call\n",
        "                0: [(1, 5, 0.0, False), #raise raise call go Round 2 => 2 raises\n",
        "                ],\n",
        "\n",
        "                #Fold\n",
        "                1: [(1, 12, -1.5, True), # raise raise fold\n",
        "                ],\n",
        "\n",
        "                #Check (Illegal)\n",
        "                2: [(1, 2, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Raise (Illegal)\n",
        "                3: [(1, 2, penalty, False) #remain to same state\n",
        "                ]\n",
        "            },\n",
        "            #Round 2 (initial state) 0 raises\n",
        "            #0 raises\n",
        "            3: {\n",
        "                #Call (illegal)\n",
        "                0: [(1, 3, penalty, False), #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Fold (illegal)\n",
        "                1: [(1, 3, penalty, False), #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Check\n",
        "                2: [((1-pair_prob)*w_prob_no_pair_high, 11, 0.5, True), #check check (win)\n",
        "                    ((1-pair_prob)*(1-w_prob_no_pair_high), 12, -0.5, True), #check check (lose)\n",
        "                    (pair_prob, 6, 0, False) #check raise\n",
        "                ],\n",
        "\n",
        "                #Raise\n",
        "                3: [(pair_prob, 7, 0 , True), #raise raise\n",
        "                    (1-pair_prob, 11, 1.5 , True) #raise fold\n",
        "                ]\n",
        "            },\n",
        "            #Round 2 (initial state) 1 raise\n",
        "            #Threshold has A or K\n",
        "            4: {\n",
        "                #Call (illegal)\n",
        "                0: [(1, 4, penalty, False), #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Fold (illegal)\n",
        "                1: [(1, 4, penalty, False), #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Check\n",
        "                2: [((1-pair_prob_high)*w_prob_high_no_pair, 10, 1.5, True), #check check (win)\n",
        "                    ((1-pair_prob_high)*(1-w_prob_high_no_pair), 11, -1.5, True), #check check (lose)\n",
        "                    ((pair_prob_high), 8, 0.0, False) #check raise\n",
        "                ],\n",
        "\n",
        "                #Raise\n",
        "                3: [(pair_prob_high, 9, 0, False), #raise raise\n",
        "                    (1-pair_prob_high, 10, 1.5, True) #raise fold\n",
        "                ]\n",
        "            },\n",
        "            #Round 2 (initial state) 2 raises\n",
        "            5: {\n",
        "                #Call (illegal)\n",
        "                0: [(1, 5, penalty, False), #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Fold (illegal)\n",
        "                1: [(1, 5, penalty, False), #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Check\n",
        "                2: [((1-pair_prob_high)*w_prob_high_no_pair, 10, 1.5, True), #check check (win)\n",
        "                    ((1-pair_prob_high)*(1-w_prob_high_no_pair), 11, -1.5, True), #check check (lose)\n",
        "                    ((pair_prob_high), 9, 0.0, False) #check raise (2 raises)\n",
        "                ],\n",
        "\n",
        "                #Raise\n",
        "                3: [(pair_prob_high, 10, 0, False), #raise raise\n",
        "                    (1-pair_prob_high, 10, 1.5, True) #raise fold\n",
        "                ]\n",
        "            },\n",
        "            #Check raise (0 raises from Round 1)\n",
        "            6: {\n",
        "                #Call\n",
        "                0: [(w_prob_pair_no_high, 11, 1.5, True), #check raise call (win)\n",
        "                    (1-w_prob_pair_no_high, 12, -1.5, True) #check raise call (lose)\n",
        "                ],\n",
        "\n",
        "                #Fold\n",
        "                1: [(1, 12, -0.5, True), #check raise fold\n",
        "                ],\n",
        "\n",
        "                #Check (Illegal)\n",
        "                2: [(1, 6, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Raise (Illegal)\n",
        "                3: [(1, 6, penalty, False) #remain to same state\n",
        "                ]\n",
        "\n",
        "            },\n",
        "            #raise raise (0 raises from round 1)\n",
        "            7: {\n",
        "                #call\n",
        "                0: [(w_prob_pair_no_high, 11, 2.5, True), #check raise call (win)\n",
        "                    (1-w_prob_pair_no_high, 12, -2.5, True) #check raise call (lose)\n",
        "                ],\n",
        "                #fold\n",
        "                1: [(1, 12, -1.5, True), #check raise fold\n",
        "                ],\n",
        "                #Check (Illegal)\n",
        "                2: [(1, 7, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Raise (Illegal)\n",
        "                3: [(1, 7, penalty, False) #remain to same state\n",
        "                ]\n",
        "                },\n",
        "            #check raise (1 raise from round 1)\n",
        "            8: {\n",
        "                #Call\n",
        "                0: [(w_prob_high_pair, 11, 2.5, True), #raise raise call (win)\n",
        "                    (1-w_prob_high_pair, 12, -2.5, True) #raise raise call (lose)\n",
        "                ],\n",
        "\n",
        "                #Fold\n",
        "                1: [(1, 12, -2.5, True), #raise raise fold\n",
        "                ],\n",
        "\n",
        "                #Check (Illegal)\n",
        "                2: [(1, 8, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Raise (Illegal)\n",
        "                3: [(1, 8, penalty, False) #remain to same state\n",
        "                ]\n",
        "\n",
        "            },\n",
        "            #raise raise (1 raise from round 1) or check raise (2 raises from round 1)\n",
        "            9: {\n",
        "                #Call\n",
        "                0: [(w_prob_high_pair, 11, 3.5, True), #raise raise call (win)\n",
        "                    (1-w_prob_high_pair, 12, -3.5, True) #raise raise call (lose)\n",
        "                ],\n",
        "\n",
        "                #Fold\n",
        "                1: [(1, 12, -2.5, True), #raise raise fold\n",
        "                ],\n",
        "\n",
        "                #Check (Illegal)\n",
        "                2: [(1, 9, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Raise (Illegal)\n",
        "                3: [(1, 9, penalty, False) #remain to same state\n",
        "                ]\n",
        "\n",
        "            },\n",
        "            #Raise raise (2 raises from Round 1)\n",
        "            10: {\n",
        "                #Call\n",
        "                0: [(w_prob_high_pair, 11, 4.5, True), #raise raise call (win)\n",
        "                    (1-w_prob_high_pair, 12, -4.5, True) #raise raise call (lose)\n",
        "                ],\n",
        "\n",
        "                #Fold\n",
        "                1: [(1, 12, -3.5, True), #raise raise fold\n",
        "                ],\n",
        "\n",
        "                #Check (Illegal)\n",
        "                2: [(1, 10, penalty, False) #remain to same state\n",
        "                ],\n",
        "\n",
        "                #Raise (Illegal)\n",
        "                3: [(1, 10, penalty, False) #remain to same state\n",
        "                ]\n",
        "            },\n",
        "\n",
        "\n",
        "            #Winning\n",
        "            11: {\n",
        "                  0: [(1, 11, 0, True)],\n",
        "                  1: [(1, 11, 0, True)],\n",
        "                  2: [(1, 11, 0, True)],\n",
        "                  3: [(1, 11, 0, True)]\n",
        "            },\n",
        "\n",
        "            #Losing\n",
        "            12: {\n",
        "                  0: [(1, 12, 0, True)],\n",
        "                  1: [(1, 12, 0, True)],\n",
        "                  2: [(1, 12, 0, True)],\n",
        "                  3: [(1, 12, 0, True)]\n",
        "            }\n",
        "        }\n",
        "     else:\n",
        "        print(\"Invalid ID Num or Opponent Type\")\n",
        "        return None\n",
        "     return P\n",
        "\n",
        "  def train(self, public_cards):\n",
        "        penalty= -1000\n",
        "\n",
        "        #calculate P state dictionary based on ID and opponent\n",
        "        P = self.get_P(penalty,public_cards)\n",
        "        if(P == None):\n",
        "            print(\"P is NoneType\")\n",
        "            exit(0)\n",
        "\n",
        "\n",
        "        #Use Policy Iteration to get optimal Value and Policy Tables\n",
        "        V_opt,P_opt = policy_iteration(P,self.gamma, self.epsilon)   #just example of calling the various new functions we created.\n",
        "\n",
        "        self.pi = P_opt\n",
        "\n",
        "\n",
        "def policy_evaluation(pi, P, gamma = 1.0, epsilon = 1e-10):  #inputs: (1) policy to be evaluated, (2) model of the environment (transition probabilities, etc., see previous cell), (3) discount factor (with default = 1), (4) convergence error (default = 10^{-10})\n",
        "    prev_V = np.zeros(len(P)) # use as \"cost-to-go\", i.e. for V(s')\n",
        "    t=0\n",
        "    while True:\n",
        "        V = np.zeros(len(P)) # current value function to be learnerd\n",
        "        for s in range(len(P)):  # do for every state\n",
        "            for prob, next_state, reward, done in P[s][pi(s)]:  # calculate one Bellman step --> i.e., sum over all probabilities of transitions and reward for that state, the action suggested by the (fixed) policy, the reward earned (dictated by the model), and the cost-to-go from the next state (which is also decided by the model)\n",
        "\n",
        "                V[s] += prob * (reward + gamma * prev_V[next_state] * (not done))\n",
        "        if np.max(np.abs(prev_V - V)) < epsilon: #check if the new V estimate is close enough to the previous one;\n",
        "            break # if yes, finish loop\n",
        "        prev_V = V.copy() #freeze the new values (to be used as the next V(s'))\n",
        "        t += 1\n",
        "    return V\n",
        "\n",
        "def policy_improvement(V, P, gamma=1.0):  # takes a value function (as the cost to go V(s')), a model, and a discount parameter\n",
        "    Q = np.zeros((len(P), len(P[0])), dtype=np.float64) #create a Q value array\n",
        "    for s in range(len(P)):        # for every state in the environment/model\n",
        "        for a in range(len(P[s])):  # and for every action in that state\n",
        "            for prob, next_state, reward, done in P[s][a]:  #evaluate the action value based on the model and Value function given (which corresponds to the previous policy that we are trying to improve)\n",
        "                Q[s][a] += prob * (reward + gamma * V[next_state] * (not done))\n",
        "    new_pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]  # this basically creates the new (improved) policy by choosing at each state s the action a that has the highest Q value (based on the Q array we just calculated)\n",
        "    # lambda is a \"fancy\" way of creating a function without formally defining it (e.g. simply to return, as here...or to use internally in another function)\n",
        "    # you can implement this in a much simpler way, by using just a few more lines of code -- if this command is not clear, I suggest to try coding this yourself\n",
        "\n",
        "    return new_pi\n",
        "\n",
        "# policy iteration is simple, it will call alternatively policy evaluation then policy improvement, till the policy converges.\n",
        "\n",
        "def policy_iteration(P, gamma = 1.0, epsilon = 1e-10):\n",
        "    random_actions = np.random.choice(tuple(P[0].keys()), len(P))     # start with random actions for each state\n",
        "    pi = lambda s: {s:a for s, a in enumerate(random_actions)}[s]     # and define your initial policy pi_0 based on these action (remember, we are passing policies around as python \"functions\", hence the need for this second line)\n",
        "    t=0\n",
        "    while True:\n",
        "        old_pi = {s: pi(s) for s in range(len(P))}  #keep the old policy to compare with new\n",
        "        #print(old_pi)\n",
        "        V = policy_evaluation(pi,P,gamma,epsilon)   #evaluate latest policy --> you receive its converged value function\n",
        "        pi = policy_improvement(V,P,gamma)          #get a better policy using the value function of the previous one just calculated\n",
        "        #print(V)\n",
        "        t += 1\n",
        "        if old_pi == {s:pi(s) for s in range(len(P))}: # you have converged to the optimal policy if the \"improved\" policy is exactly the same as in the previous step\n",
        "            #print(old_pi)\n",
        "            break\n",
        "    #print('converged after %d iterations' %t) #keep track of the number of (outer) iterations to converge\n",
        "    return V,old_pi\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "id": "xMOUmkECEUSP",
        "outputId": "df510593-63cb-4cdb-9318-107aa9303c08"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-2a5444c1e72c>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m           \u001b[0mcurr_reward\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m           \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcurr_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-e65172b23f68>\u001b[0m in \u001b[0;36mplayGame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0mpre_public_cards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdealer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeal_card\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdealer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeal_card\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_public_cards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-e65172b23f68>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, public_cards)\u001b[0m\n\u001b[1;32m   1860\u001b[0m         \u001b[0;31m#through experimentation gamma = 0.99 seems to have the biggest sum of V matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1862\u001b[0;31m         \u001b[0mV_opt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mP_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#just example of calling the various new functions we created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m         \u001b[0;31m#print(V_opt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-e65172b23f68>\u001b[0m in \u001b[0;36mpolicy_iteration\u001b[0;34m(P, gamma, epsilon)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         \u001b[0mold_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m#keep the old policy to compare with new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m         \u001b[0;31m#print(old_pi)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m         \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#evaluate latest policy --> you receive its converged value function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m         \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_improvement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m#get a better policy using the value function of the previous one just calculated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m#print(V)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-e65172b23f68>\u001b[0m in \u001b[0;36mpolicy_evaluation\u001b[0;34m(pi, P, gamma, epsilon)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                \u001b[0;31m# print(reward)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                \u001b[0;31m# print(done)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m                 \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprev_V\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1881\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_V\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#check if the new V estimate is close enough to the previous one;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m             \u001b[0;32mbreak\u001b[0m \u001b[0;31m# if yes, finish loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "############### Policy Iteration experiments ##################\n",
        "\n",
        "#Compare different gammas for all kinds of opponents and player ids\n",
        "gammas = [0, 0.1, 0.5, 0.9, 0.99]\n",
        "player_ids = [1, 2]\n",
        "opp_types = [\"Random\", \"Threshold\"]\n",
        "for player_id in player_ids:\n",
        "  for opp_type in opp_types:\n",
        "    for gamma in gammas:\n",
        "      if(player_id == 1):\n",
        "        if(opp_type == \"Random\"):\n",
        "          pk = PokerGame(PolicyAgent(player_id, opp_type, gamma), RandomAgent())\n",
        "        else:\n",
        "          pk = PokerGame(PolicyAgent(player_id, opp_type, gamma), ThresholdAgent())\n",
        "          pk.threshold2 = True\n",
        "        pk.policy1 = True\n",
        "      else:\n",
        "        if(opp_type == \"Random\"):\n",
        "          pk = PokerGame(RandomAgent(), PolicyAgent(player_id, opp_type, gamma))\n",
        "        else:\n",
        "          pk = PokerGame(ThresholdAgent(), PolicyAgent(player_id, opp_type, gamma))\n",
        "        pk.policy2 = True\n",
        "\n",
        "      games = 1000\n",
        "      cum_reward = np.zeros(games)\n",
        "      reward = np.zeros(games)\n",
        "      for i in range(games):\n",
        "        if(player_id == 1):\n",
        "          curr_reward , _ = pk.playGame()\n",
        "        else:\n",
        "          _ , curr_reward = pk.playGame()\n",
        "        reward[i] = curr_reward\n",
        "      cum_reward = np.cumsum(reward)\n",
        "      games_idx = np.arange(0, games)\n",
        "\n",
        "      # Plot the first graph\n",
        "      plt.plot(games_idx, cum_reward, label='Gamma = ' +str(gamma))\n",
        "\n",
        "    # Add labels and a legend\n",
        "    plt.title('Policy as player ' +str(player_id) +' vs ' +opp_type)\n",
        "    plt.xlabel('Games')\n",
        "    plt.ylabel('Cummulative Reward')\n",
        "    plt.legend()\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#Compare random vs threshold considering their best gamma\n",
        "epsilon = 0.0000001\n",
        "player_ids = [1,2]\n",
        "opp_types = [\"Random\", \"Threshold\"]\n",
        "for player_id in player_ids:\n",
        "    for opp_type in opp_types:\n",
        "        if(player_id == 1):\n",
        "            if(opp_type == \"Random\"):\n",
        "                pk = PokerGame(PolicyAgent(player_id, opp_type, 0.9, epsilon), RandomAgent())\n",
        "            else:\n",
        "                pk.threshold2 = True\n",
        "                pk = PokerGame(PolicyAgent(player_id, opp_type, 0.1, epsilon), ThresholdAgent())\n",
        "            pk.policy1 = True\n",
        "        else:\n",
        "            if(opp_type == \"Random\"):\n",
        "                pk = PokerGame(RandomAgent(), PolicyAgent(player_id, opp_type, 0.9, epsilon))\n",
        "            else:\n",
        "                pk = PokerGame(ThresholdAgent(), PolicyAgent(player_id, opp_type, 0.1, epsilon))\n",
        "            pk.policy2 = True\n",
        "\n",
        "        games = 3000\n",
        "        cum_reward = np.zeros(games)\n",
        "        reward = np.zeros(games)\n",
        "        for i in range(games):\n",
        "            if(player_id == 1):\n",
        "                curr_reward , _ = pk.playGame()\n",
        "            else:\n",
        "                 _ , curr_reward = pk.playGame()\n",
        "            reward[i] = curr_reward\n",
        "        cum_reward = np.cumsum(reward)\n",
        "        games_idx = np.arange(0, games)\n",
        "\n",
        "        # Plot the first graph\n",
        "        plt.plot(games_idx, cum_reward, label='Policy vs ' +opp_type)\n",
        "\n",
        "        print(\"Mean Reward: \"+str(sum(reward)/games))\n",
        "    display(Markdown(\n",
        "\n",
        "              f\"### Mean Rewards: \" +\n",
        "              f\"$\\mu_1$={mean_reward[0]}, \" +\n",
        "              f\"$\\mu_2$={mean_reward[1]} \"\n",
        "\n",
        "    ))\n",
        "    # Add labels and a legend\n",
        "    plt.title('Policy as player ' +str(player_id))\n",
        "    plt.xlabel('Games')\n",
        "    plt.ylabel('Cummulative Reward')\n",
        "    plt.legend()\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ipduRJMG08UI",
        "outputId": "24051682-7161-467a-9323-46088dae7db5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-006ef80f031a>\u001b[0m in \u001b[0;36m<cell line: 148>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopp_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Random\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m           \u001b[0mq_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQLearningAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m           \u001b[0mpk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPokerGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m           \u001b[0mq_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQLearningAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-67623d7ed3ce>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, player1, player2, feedback)\u001b[0m\n\u001b[1;32m    207\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeedback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;31m#Players, judger, dealer initializations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdealer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjudger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPokerJudger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "#Compare different epsilons for all kinds of opponents and player ids\n",
        "epsilons = [0, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
        "player_ids = [1, 2]\n",
        "opp_types = [\"Random\", \"Threshold\"]\n",
        "for player_id in player_ids:\n",
        "  for opp_type in opp_types:\n",
        "    for epsilon in epsilons:\n",
        "      if(player_id == 1):\n",
        "        if(opp_type == \"Random\"):\n",
        "          q_agent = QLearningAgent(True, 0.1, 0.1, epsilon)\n",
        "          pk = PokerGame(q_agent, RandomAgent())\n",
        "        else:\n",
        "          q_agent = QLearningAgent(True, 0.1, 0.1, epsilon)\n",
        "          pk = PokerGame(q_agent, ThresholdAgent())\n",
        "        pk.q1 = True\n",
        "      else:\n",
        "        if(opp_type == \"Random\"):\n",
        "          q_agent = QLearningAgent(True, 0.1, 0.1, epsilon)\n",
        "          pk = PokerGame(RandomAgent(), q_agent)\n",
        "        else:\n",
        "          q_agent = QLearningAgent(True, 0.5, 0.1, epsilon)\n",
        "          pk = PokerGame(ThresholdAgent(), q_agent)\n",
        "        pk.q2 = True\n",
        "\n",
        "      num_episodes = 10000\n",
        "      for episode in range(num_episodes):\n",
        "          # Play a game\n",
        "          pk.playGame()\n",
        "\n",
        "          # Update the Q-values using the rewards obtained in the game\n",
        "          for i in range(len(pk.states)):\n",
        "              state = pk.states[i]\n",
        "              action = pk.actions[i]\n",
        "              next_state = pk.states[i + 1] if i + 1 < len(pk.states) else None\n",
        "              reward = pk.rewards[i]\n",
        "              q_agent.update_q_table(state, action, next_state, reward)\n",
        "\n",
        "          # Reset the game\n",
        "          pk.states = []\n",
        "          pk.actions = []\n",
        "          pk.rewards = []\n",
        "\n",
        "      if(player_id == 1):\n",
        "        pk.player1.q_learn = False\n",
        "      else:\n",
        "        pk.player2.q_learn = False\n",
        "\n",
        "      games = 10000\n",
        "      cum_reward = np.zeros(games)\n",
        "      reward = np.zeros(games)\n",
        "      for i in range(games):\n",
        "        if(player_id == 1):\n",
        "          curr_reward , _ = pk.playGame()\n",
        "        else:\n",
        "          _ , curr_reward = pk.playGame()\n",
        "        reward[i] = curr_reward\n",
        "      cum_reward = np.cumsum(reward)\n",
        "      games_idx = np.arange(0, games)\n",
        "\n",
        "      # Plot the first graph\n",
        "      plt.plot(games_idx, cum_reward, label='Epsilon = ' +str(epsilon))\n",
        "\n",
        "    # Add labels and a legend\n",
        "    plt.title('Q Learning as player ' +str(player_id) +' vs ' +opp_type)\n",
        "    plt.xlabel('Games')\n",
        "    plt.ylabel('Cummulative Reward')\n",
        "    plt.legend()\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#Compare random vs threshold considering their best aplha, gamma, epsilon\n",
        "\n",
        "for player_id in player_ids:\n",
        "  mean_reward = []\n",
        "  best_gamma = 0.1\n",
        "  for opp_type in opp_types:\n",
        "    if(player_id == 1):\n",
        "      if(opp_type == \"Random\"):\n",
        "        best_alpha = 0.1\n",
        "        best_epsilon = 0.01\n",
        "        q_agent = QLearningAgent(True, best_alpha, best_gamma, best_epsilon)\n",
        "        pk = PokerGame(q_agent, RandomAgent())\n",
        "      else:\n",
        "        best_alpha = 0.1\n",
        "        best_epsilon = 0.1\n",
        "        q_agent = QLearningAgent(True, best_alpha, best_gamma, best_epsilon)\n",
        "        pk = PokerGame(q_agent, ThresholdAgent())\n",
        "      pk.q1 = True\n",
        "    else:\n",
        "      if(opp_type == \"Random\"):\n",
        "        best_alpha = 0.1\n",
        "        best_epsilon = 0.05\n",
        "        q_agent = QLearningAgent(True, best_alpha, best_gamma, best_epsilon)\n",
        "        pk = PokerGame(RandomAgent(), q_agent)\n",
        "      else:\n",
        "        best_alpha = 0.5\n",
        "        best_epsilon = 0.001\n",
        "        q_agent = QLearningAgent(True, best_alpha, best_gamma, best_epsilon)\n",
        "        pk = PokerGame(ThresholdAgent(), q_agent)\n",
        "      pk.q2 = True\n",
        "\n",
        "    num_episodes = 10000\n",
        "    for episode in range(num_episodes):\n",
        "        # Play a game\n",
        "        pk.playGame()\n",
        "\n",
        "        # Update the Q-values using the rewards obtained in the game\n",
        "        for i in range(len(pk.states)):\n",
        "            state = pk.states[i]\n",
        "            action = pk.actions[i]\n",
        "            next_state = pk.states[i + 1] if i + 1 < len(pk.states) else None\n",
        "            reward = pk.rewards[i]\n",
        "            q_agent.update_q_table(state, action, next_state, reward)\n",
        "\n",
        "        # Reset the game\n",
        "        pk.states = []\n",
        "        pk.actions = []\n",
        "        pk.rewards = []\n",
        "\n",
        "    if(player_id == 1):\n",
        "      pk.player1.q_learn = False\n",
        "    else:\n",
        "      pk.player2.q_learn = False\n",
        "\n",
        "    games = 10000\n",
        "    cum_reward = np.zeros(games)\n",
        "    reward = np.zeros(games)\n",
        "    for i in range(games):\n",
        "      if(player_id == 1):\n",
        "        curr_reward , _ = pk.playGame()\n",
        "      else:\n",
        "        _ , curr_reward = pk.playGame()\n",
        "      reward[i] = curr_reward\n",
        "    mean_reward.append(np.mean(reward))\n",
        "    cum_reward = np.cumsum(reward)\n",
        "    games_idx = np.arange(0, games)\n",
        "\n",
        "    # Plot the first graph\n",
        "    plt.plot(games_idx, cum_reward, label=str(opp_type) +'\\nalpha = ' +str(best_alpha) +'\\ngamma = ' +str(best_gamma) +'\\nepsilon = ' +str(best_epsilon))\n",
        "\n",
        "  display(Markdown(\n",
        "\n",
        "                f\"### Mean Rewards: \" +\n",
        "                f\"$\\mu_1$={mean_reward[0]}, \" +\n",
        "                f\"$\\mu_2$={mean_reward[1]} \"\n",
        "\n",
        "    ))\n",
        "\n",
        "  # Add labels and a legend\n",
        "  plt.title('Q Learning as player '+str(player_id))\n",
        "  plt.xlabel('Games')\n",
        "  plt.ylabel('Cummulative Reward')\n",
        "  plt.legend()\n",
        "\n",
        "  # Display the plot\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "2MTg99x2XHtb",
        "outputId": "a9a9b96e-f390-4c82-afa1-f7c32e736c88"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-9160b193dceb>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Play a game\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mpk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Update the Q-values using the rewards obtained in the game\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-05fc3c313b6b>\u001b[0m in \u001b[0;36mplayGame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwin_chips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_chips\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwin_chips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_chips\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwin_chips\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PokerGame' object has no attribute 'q_train'"
          ]
        }
      ],
      "source": [
        "#Compare policy vs q learning\n",
        "player_ids = [1, 2]\n",
        "opp_types = [\"Random\", \"Threshold\"]\n",
        "\n",
        "for player_id in player_ids:\n",
        "  for opp_type in opp_types:\n",
        "    #Q Learning\n",
        "    best_gamma = 0.1\n",
        "    if(player_id == 1):\n",
        "      if(opp_type == \"Random\"):\n",
        "        best_alpha = 0.1\n",
        "        best_epsilon = 0.01  #0.01\n",
        "        q_agent = QLearningAgent(True, best_alpha, best_gamma, best_epsilon)\n",
        "        pk = PokerGame(q_agent, RandomAgent())\n",
        "      else:\n",
        "        best_alpha = 0.1\n",
        "        best_epsilon = 0.01  #0.1\n",
        "        q_agent = QLearningAgent(True, best_alpha, best_gamma, best_epsilon)\n",
        "        pk = PokerGame(q_agent, ThresholdAgent())\n",
        "      pk.q1 = True\n",
        "    else:\n",
        "      if(opp_type == \"Random\"):\n",
        "        best_alpha = 0.1\n",
        "        best_epsilon = 0.01  #0.005\n",
        "        q_agent = QLearningAgent(True, best_alpha, best_gamma, best_epsilon)\n",
        "        pk = PokerGame(RandomAgent(), q_agent)\n",
        "      else:\n",
        "        best_alpha = 0.1   #0.5\n",
        "        best_epsilon = 0.01   #0.001\n",
        "        q_agent = QLearningAgent(True, best_alpha, best_gamma, best_epsilon)\n",
        "        pk = PokerGame(ThresholdAgent(), q_agent)\n",
        "      pk.q2 = True\n",
        "\n",
        "    num_episodes = 1000\n",
        "    for episode in range(num_episodes):\n",
        "        # Play a game\n",
        "        pk.playGame()\n",
        "\n",
        "        # Update the Q-values using the rewards obtained in the game\n",
        "        for i in range(len(pk.states)):\n",
        "            state = pk.states[i]\n",
        "            action = pk.actions[i]\n",
        "            next_state = pk.states[i + 1] if i + 1 < len(pk.states) else None\n",
        "            reward = pk.rewards[i]\n",
        "            q_agent.update_q_table(state, action, next_state, reward)\n",
        "\n",
        "        # Reset the game\n",
        "        pk.states = []\n",
        "        pk.actions = []\n",
        "        pk.rewards = []\n",
        "\n",
        "    if(player_id == 1):\n",
        "      pk.player1.q_learn = False\n",
        "    else:\n",
        "      pk.player2.q_learn = False\n",
        "\n",
        "    games = 1000\n",
        "    cum_reward = np.zeros(games)\n",
        "    reward = np.zeros(games)\n",
        "    for i in range(games):\n",
        "      if(player_id == 1):\n",
        "        curr_reward , _ = pk.playGame()\n",
        "      else:\n",
        "        _ , curr_reward = pk.playGame()\n",
        "      reward[i] = curr_reward\n",
        "    mean_reward.append(np.mean(reward))\n",
        "    cum_reward = np.cumsum(reward)\n",
        "    games_idx = np.arange(0, games)\n",
        "\n",
        "    # Plot the first graph\n",
        "    plt.plot(games_idx, cum_reward, label='Q Learning agent')\n",
        "\n",
        "    #Policy\n",
        "    if(player_id == 1):\n",
        "      if(opp_type == \"Random\"):\n",
        "        pk = PokerGame(PolicyAgent(player_id, opp_type, gamma), RandomAgent())\n",
        "      else:\n",
        "        pk = PokerGame(PolicyAgent(player_id, opp_type, gamma), ThresholdAgent())\n",
        "        pk.threshold2 = True\n",
        "      pk.policy1 = True\n",
        "    else:\n",
        "      if(opp_type == \"Random\"):\n",
        "        pk = PokerGame(RandomAgent(), PolicyAgent(player_id, opp_type, gamma))\n",
        "      else:\n",
        "        pk = PokerGame(ThresholdAgent(), PolicyAgent(player_id, opp_type, gamma))\n",
        "      pk.policy2 = True\n",
        "\n",
        "    games = 1000\n",
        "    cum_reward = np.zeros(games)\n",
        "    reward = np.zeros(games)\n",
        "    for i in range(games):\n",
        "      if(player_id == 1):\n",
        "        curr_reward , _ = pk.playGame()\n",
        "      else:\n",
        "        _ , curr_reward = pk.playGame()\n",
        "      reward[i] = curr_reward\n",
        "    cum_reward = np.cumsum(reward)\n",
        "    games_idx = np.arange(0, games)\n",
        "\n",
        "    # Plot the first graph\n",
        "    plt.plot(games_idx, cum_reward, label='Policy agent')\n",
        "\n",
        "    # Add labels and a legend\n",
        "    plt.title('Player '+str(player_id) +' against '+opp_type)\n",
        "    plt.xlabel('Games')\n",
        "    plt.ylabel('Cummulative Reward')\n",
        "    plt.legend()\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}